[{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/install_guide/wsl/","title":"[WIN ONLY] Windows Subsystem for Linux","tags":[],"description":"","content":" Overview Check your windows version Windows version larger than (or equal to) 19041 Windows version smaller than 19041 Verifying your install Overview Windows now has the ability to install a linux operating system on your machine without the use of an emulator. This gives you a full-featured linux environment that can interact with your normal files.\nCheck your windows version First, please check the build version of Windows that you are using. This can be done by selecting the Windows logo key + r on your keyboard. Once you do that, you should see the windows Run prompt:\nType the command \u0026ldquo;winver\u0026rdquo; (no quotes) into the prompt, as depicted in the image above, and hit enter. You should see a screen similar to this:\nYour build version number is the one that follows the \u0026ldquo;Windows Build\u0026rdquo; text (as highlighted in the above image). Depending on whether your build number is larger or smaller than 19041, please follow the appropriate directions below:\nLarger than (or equal to) 19041 Smaller than 19041 Windows version larger than (or equal to) 19041 In the start menu, search for \u0026ldquo;Powershell\u0026rdquo;, right click and select \u0026ldquo;run as administrator\u0026rdquo;.\nA small blue window should open with a flashing cursor. Please type wsl --install and hit enter:\nThis will take a few minutes to install everything. Once it is finished, please skip to verifying your install.\nIf the above installation did not work- please try to install [using these instructions][Windows version larger smaller than 19041] (even if your version is not smaller than 19041).\nWindows version smaller than 19041 In the start menu, search for \u0026ldquo;Turn Windows features on or off\u0026rdquo; and open that settings window.\nIn the settings window, scroll down to \u0026ldquo;Windows Subsystem for Linux\u0026rdquo;, check the box next to it, and select OK at the bottom of the window.\nYou will not need to restart your computer. Once you have rebooted, open the Windows Store from the start menu.\nIn the Windows Store, search for Ubuntu, and select the version-less one unless you have a reason to pick a specific version.\nOn the Ubuntu page, select Get to install.\nVerifying your install In the start menu, search for and run Ubuntu.\nIf a terminal window opens, you should be good to go! You can also open this terminal inside any folder on your computer by holding shift and right-clicking, then selecting \u0026ldquo;Open Linux shell here.\u0026rdquo;\nThis creates an entirely new operating system on your machine. Thus, things like your git configuration and SSH key for Github will not carry over! You will need to configure git again, and create a new SSH key for this operating system.\nThanks to the UC Davis DataLab\u0026rsquo;s Install Guide for providing a portion of this guide.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/install_guide/r_rstudio/","title":"R and R Studio","tags":[],"description":"","content":" Overview Common Settings Overview R is a programming language and computing environment specialized for statistical analysis and data manipulation. It\u0026rsquo;s commonly used for performing statistical tests, creating data visualizations, and writing data analysis reports. Despite focusing on statistics, it\u0026rsquo;s a full-fledged programming language, and relatively easy to learn.\nYou should have gotten R and R studio install on the first data of SDS 100. If you did not, please follow the guide here.\nCommon Settings There are a few settings I recommend changing in R studio to make the process of working with it a little easier. In the top bar, click on Tools \u0026gt; Global Options and modify the following.\nUnder General \u0026gt; Basic \u0026gt; Workspace, disable \u0026ldquo;Restore .RData into workspace at startup.\u0026rdquo; Under General \u0026gt; Basic \u0026gt; Workspace, set \u0026ldquo;Save workspace to .RData on exit\u0026rdquo; to Never. Under Code \u0026gt; Editing, enable \u0026ldquo;Soft-wrap R source files.\u0026rdquo; Under Code \u0026gt; Display, enable \u0026ldquo;Show Margin\u0026rdquo; with \u0026ldquo;Margin Column\u0026rdquo; set to 80. Under Code \u0026gt; Display, enable \u0026ldquo;Highlight R Function Calls.\u0026rdquo; Under Code \u0026gt; Display, enable \u0026ldquo;Rainbow Parenthesis.\u0026rdquo; Under Code \u0026gt; Display \u0026gt;, enable \u0026ldquo;Enable preview of named colors and hexadecimal colors.\u0026rdquo; Under R Markdown \u0026gt; Basic, disable \u0026ldquo;Show output inline for all R Markdown documents.\u0026rdquo; Under R Markdown \u0026gt; Visual, disable \u0026ldquo;Use visual editor by default for new documents.\u0026rdquo; Under Appearance, pick a theme you like! [WIN ONLY] Under Terminal \u0026gt; General, set \u0026ldquo;New terminals open with\u0026rdquo; to \u0026ldquo;Bash\u0026rdquo; (You can only do this after you complete the install guide 0: Windows Subsystem for Linux) "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/syllabus/","title":"Syllabus","tags":[],"description":"","content":" Course Description Course Structure Course Instructor Contacting Me Course Policies Required Materials Attendance Academic Honesty Code of Conduct Accommodation Grading Standards-Based Grading Standards Final Grades Late Work Policy FAQ Course Description The primary aim of Advanced Programming for Data Science (SDS 270) is to teach how to use R to do things. The course will build programming competencies, culminating in students creating their own code packages for R in a group. We will start by creating single functions, learning to debug them, then incrementally add more elements of the programming life cycle such as project management, automatic code testing, and building proper documentation. Supplemental skills useful for advanced programming such as using remote servers, as well as various methods for creating your own data sets will be explored in support of creating the final R packages.\nCourse Structure Each week follows the same basic structure. Monday and Wednesday classes include lectures to introduce new concepts. Each lecture is followed by interactive problem sets designed to reinforce concepts through active learning. Slides from lecture will be posted online after class. The problem sets for any class are \u0026ldquo;due\u0026rdquo; at the start of the next class period when the answers will be released; most problem sets can be completed in class. In-class problem sets do not contribute toward your grade. They are intended to reinforce material and help you test your own understanding.\nFriday classes are devoted to lab activities or project work time. Students are expected to come to class for these activities. Labs include more involved problem sets that incorporate topics from the current and prior weeks. Students work on labs in groups of two to four people. Labs are reviewed through GitHub Classroom where feedback is provided.\nFor a full list of assignments and due dates, please see the course schedule.\nThis is a 4-credit course. You should be spending 12-hours total per week on this course. Expect to spend around 8.25 hours (12 hours - 3.75 hours/week of in-class instruction) on class material per week outside of class.\nCourse Instructor I am a sociologist that studies abuses of power in government. I earned my Ph.D.Â at the University of California, Davis in in sociology with a designated emphasis in computational social science. I combine computational methods such as social network analysis, natural language processing, geospatial analysis, and machine learning with open source and governmental data to uncover patterns of malfeasance and misfeasance by our public servants. From the political networks of politicians and prohibition gangsters to bias hidden in the text of academic recruitment, I use new methods to work on old problems of corruption and inequality.\nI am a visiting assistant professor in the Statistical \u0026amp; Data Sciences (SDS) program. I have experience working with both United States and United Kingdom governmental organizations applying machine learning to real-world problems. In the UK, I worked with the national lab for data science and machine learning, the Alan Turing Institute, on early-detection systems in foster care to assure children are receiving adequate services. Meanwhile in the US I worked with the Internal Revenue Service to build a machine learning system that determined the credibility of incoming fraud reports.\nContacting Me Slack Office Hours You can send me a message on the course Slack workspace, and I will respond when I am able, typically within 24 hours during the work week. To message me, click the + button next to \u0026ldquo;Direct Messages\u0026rdquo; and search for my name.\nIf your question is not sensitive in nature, consider putting it in the #coding-help or #course-help channel instead. There is a good chance one of your classmates will be able to answer before I can.\nSlack questions should be brief or administrative in nature. For more in-depth questions and troubleshooting please attend office hours.\nYou can schedule a meeting with me on Calendly. Drop-ins are welcome, but priority is given to those who make an appointment. Group appointments, to address a similar question, are welcome.\nIf you are coming to office hours with a coding question, make sure you have the code ready at the start of your appointment. Have your computer booted up and your project open.\nIf you cannot find an open time slot, please message me for an appointment. I will attempt to find a time that works for both of us.\nCourse Policies Required Materials Students are not expected to buy any materials for this course. Data science is built on free and open collaboration. There is no shortage of high-quality learning material available. This reader, as well as all assignments, are currently available for free.\nStudents are required to have a working computer (preferably a laptop) and reliable internet connection for this course. Any recent computer should be sufficient, with the notable exception of Chromebooks. Chromebooks lack access to the majority of the tools used by data scientists.\nIf you only have access to a Chromebook, please speak with me as soon as possible.\nAttendance I will not be taking attendance in this course, and you do not need to inform me when you will be absent. If you are sick, please stay home. Given the standards-based grading system (discussed below), no single class, assignment, or even quiz will negatively impact your grade. That said, it will be very difficult to keep up with course material without consistent attendance.\nIf you miss a class, you should contact a peer to discuss what was missed, and check the course reader website for any upcoming deadlines. I won\u0026rsquo;t have the capacity to re-deliver missed material in office hours.\nQuizzes cannot be made up after the open period has passed. If you have a known scheduling conflict with a quiz, please speak with me as soon as possible to arrange an alternative time.\nPlease see the SDS department\u0026rsquo;s official policy regarding remote learning:\nIn keeping with Smith\u0026rsquo;s core identity and mission as an in-person, residential college, the Program in Statistical \u0026amp; Data Sciences affirms College policy (as articulated by Provost Michael Thurston and Dean of the College Alex Keller) that students will attend class in person. As such, SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Office of Disability Services will be the only exceptions to this policy. As with any other kind of accommodations under the Americans with Disabilities Act (ADA), please notify your instructor during the first week of classes to schedule a meeting with them to discuss how we can work with you to provide the most accessible course possible.\nAcademic Honesty Data science is inherently collaborative, so I fully expect students to collaborate. You are encouraged to work together on most assignments\u0026mdash;ask questions on Slack, create study groups, and share helpful resources you find. However, anything you submit must be your own work. You need to be the person who writes the text and/or code. Multiple students should not submit identical work. Please note: The only avenue in which collaboration is not allowed is on quizzes.\nAll students, staff, and faculty are bound by the Smith College Honor Code:\nStudents and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information and honest and respectful use of college resources.\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. All submitted work of any kind must be the original work of the student who must cite all the sources used in its preparation.\n-Smith Academic Honor Code Any cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\nSubmitting work completed by another student as your own. Copying and pasting text or code from sources without quoting and citing the author. Paraphrasing material from another source without citing the author. Failing to cite your sources correctly. Falsifying or misrepresenting information in submitted work. Having another student or service complete assignments for you. Learning to code is similar to learning a new language; you will only learn by doing. No amount of rote copying will advance you beyond the most elementary levels of understanding. Please keep this in mind.\nIf someone else helps you understand a concept better, give them a nod in the #shoutouts channel on Slack.\nCode of Conduct As participants in this course we are committed to making participation a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned with this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant.\nAccommodation Smith College is committed to providing support services and reasonable accommodations to all students with disabilities. To request an accommodation, please register with the Office of Disability Services (ODS) at the beginning of the semester.\nGrading Standards-Based Grading This course will be graded using a standards-based grading system. Rather than tallying up the percentage of questions you answer correctly, I assess your responses by using a pre-defined set of course standards and then assign a level of proficiency. Throughout the semester, this course offers multiple opportunities to showcase the depth of your understanding in light of these standards.\nIn traditional points-style grading, an average is taken of all your assignments, and your final grade is based on that average. This means all assignments are given equal consideration in your final grade.\nIn contrast, standards-based grading is focused on your progression through the course. Functionally, only your best score for each standard is kept. All others are effectively forgotten. The hope is that without the worry of \u0026ldquo;getting a bad grade\u0026rdquo; when you are new to a concept, you will feel free to safely engage with complicated topics early on, make mistakes, and have opportunities to show improvement without penalization.\nA standards-based grading system carries a number of other benefits:\nLearning targets for the course are clearly defined from the outset. Every graded assignment is directly tied to at least one standard. There is no \u0026ldquo;busy work\u0026rdquo; with a standards-based system. No single assignment will make-or-break your grade. You have multiple opportunities to demonstrate fluency in a standard. This rewards students that take the time to practice and learn from their mistakes. It prioritizes student growth throughout the course of the semester. Assessments in a standards-based system are much clearer than in a point-based grading system. Saying that I\u0026rsquo;ve become proficient in doing X, Y, or Z means more than saying that I earned a 92.5 in my course. A standards-based grading system makes it easier to monitor your own progress towards a certain grade. There is no competition and no curve in a standards-based system. The only person you are ever compared with is your past self. Help each other often and freely. Standards The following table lists all the standards you are evaluated on in this course. There are 8 total standards, separated into 5 categories. Each standard states what conditions must be met to reach each proficiency level. There are four proficiency levels for each standard, each requiring more complete understanding of the material. These levels are inclusive, meaning to reach \u0026ldquo;Exceeds Standard\u0026rdquo; you must also meet all the requirements of \u0026ldquo;Meets Standard\u0026rdquo; and below.\nYou will have multiple opportunities to demonstrate your understanding of each standard. Any assignment that is reviewed is an opportunity to increase your proficiency level in a standard. In addition to the levels of proficiency, there is also an extra point available in each standard called \u0026ldquo;Individual Standard.\u0026rdquo; You may fulfill this requirement only on quizzes, but only need to reach the \u0026ldquo;Meets Standard\u0026rdquo; criteria on a standard to do so.\nYou can demonstrate proficiency in any reviewed assignment, but can only fulfill the \u0026ldquo;Individual Standard\u0026rdquo; criteria on a quiz.\nStandard Does Not Meet Standard Progressing Toward Standard Near Standard Meets Standard Exceeds Standard Individual Standard Functions Copies-and-pastes similar code with small changes. Creates simple functions with predetermined inputs. Creates functions that that make use of arguments to modify their output. Creates functions that can handle novel inputs, with logic and error checking to determine outputs. Creates functions that can handle arbitrary input through the use of \". . .\" or S3 classes. Iteration Copies-and-pastes similar code several times to repeat identical tasks on different data. Calls an identical function (even self-made) to repeat a task on different data. Uses loop style iteration to act on sequences of data. Uses apply style iteration to act on sequences of data. Can iterate in parallel and justify the overhead costs. Data Structures Cannot differentiate between data types (numeric, logical, character, etc.). Uses the wrong type during analyses. Can subset data from vectors or dataframes by position or condition. Works with lists when appropriate and can subset list elements effectively by position or condition. Works with attributes effectively. Can modify and use attributes to manipulate or create data structures. Works with lists by inputting and sub-setting programmatically defined arbitrary elements. Code Style Code style is inconsistent and lacks appropriate comments. Code comments explain the general purpose of the code file. Indentation is consistent and predictable. Code comments explain the broad strokes of intended behavior per section. Files make use of the built-in section headings in R Studio. Comments explain code step-by-step. All dependencies and parameters are clearly provided and explained at the top of code files. Code includes built in error checking for anticipated problems. These checks will output warnings or stop execution of code. git/Github Does not use git for version control. Uses git for version control on individual projects with appropriately sized commits with descriptive commit messages. Uses git and GitHub for version control on group work with appropriately sized commits with descriptive commit messages. Effectively creates and merges branches of appropriate size. No sensitive files are committed. Effectively requests and performs code review on pull requests using GitHub. Project Management Does not use project management tools. Project management is only conducted though messaging, such as on slack. User the GitHub issue tracker to set milestones, but does not fill out associated metadata such as who is responsible for the issue and when it is due. Uses the GitHub issue tracker to break down and assign actionable work with all appropriate metadata included. Uses projects KanBan boards to break down tasks into actionable steps, and track their completion. Package Function Package cannot be installed. Package does not pass check(). One or more component functions or unit tests of package do not execute. Package includes and passes all unit tests to ensure expected functionality. Package passes check() with no warnings. Package Documentation Package has no documentation. Team can create a ReadMe file which explains the overall purpose of the package and its use. Package includes help files for all functions, including a definition, expected inputs, outputs, and clear descriptions of arguments. Vignettes are provided for the major functionalities of the package. Help files include reproducible examples of function usage. A live website is accessible for the package to share resources and vignettes. Final Grades Your completion of these standards are converted into a final letter grade using the following process. Each of the 8 standards will be converted into a five-point scale, with one point available for meeting the \u0026ldquo;Individual Standard\u0026rdquo; on a quiz.\n1 Point. \u0026ldquo;Does Not Meet Standard\u0026rdquo; 2 Points. \u0026ldquo;Progressing Toward Standard\u0026rdquo; 3 Points. \u0026ldquo;Near Standard\u0026rdquo; 4 Points. \u0026ldquo;Meets Standard\u0026rdquo; 5 Points. \u0026ldquo;Exceeds Standard\u0026rdquo; +1 Point. \u0026ldquo;Individual Standard\u0026rdquo; On this scale, there are 48 points total in the course (8 standards * 6 possible points). I sum the highest level of proficiency you reach in each standard over the course of the semester to arrive at your final score. For example, if someone were to reach \u0026ldquo;Exceeds Standard\u0026rdquo; in all standards, but could never do so on a quiz, they would receive 40 of 48 points (5 points * 8 standards). Similarly, if someone reaches \u0026ldquo;Meets Standard\u0026rdquo; in all topics, including on quizzes, but did not reach \u0026ldquo;Exceeds Standard\u0026rdquo; in any topic, they would likewise receive 40 of 48 points.\nThe summed points will be converted into letter grades using the following table.\nLetter Points A 45, 46, 47, 48 A- 43, 44 B+ 41, 42 B 39, 40 B- 37, 38 C+ 35, 36 C 33, 34 C- 31, 32 D+ 29, 30 D 27, 28 D- 25, 26 F 0 - 24 Late Work Policy Assignments turned in late will not be reviewed, and will not be considered for demonstrating proficiency in course standards. Keep in mind, missing an assignment will not hurt your grade, but does remove one chance for you to demonstrate your knowledge of course material. If you do not think you will be able to turn in an assignment by the deadline, you may request an extension. To do so, please send me a message explaining why you are unable to complete the assignment in the expected time frame. Extension requests must be made\u0026ndash;and accepted\u0026ndash;before the assignment due date.\nAfter the due date, late assignments are only reviewed if there are emergency circumstances preventing you from turning the assignment in on time.\nFAQ Q: So if I reach \u0026ldquo;Exceeds Standard\u0026rdquo; and fulfill the individual standard on a quiz for a topic early in the semester, I can just skip those questions for the rest of the class?\nA: Theoretically yes, but I would recommend you answer all questions to make sure you\u0026rsquo;re not letting your knowledge slip.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/02_git/","title":"git","tags":[],"description":"","content":" Overview A Simple Repo 1. Make a New Project 2. Your First Commit 3. Making a Change 4. Adding More Files 5. Breaking Things 6. Time Travel for Beginners 7. To the Cloud 8. Conflict Conclusion Overview git is remarkably useful, but it takes an investment in using it well to gain the full benefits. Today we\u0026rsquo;re going to try and develop some good habits. While they sometimes feel like a chore to adhere to, if you continue on in data science, good git practice will save you some day. It saved me from losing days worth of progress on a research paper when my file syncing service corrupted my files.\nA Simple Repo 1. Make a New Project git store information in a special file structure called a repository, commonly called the repo. Under the hood, this is really just a folder that is organized in a very specific way on your computer. While you can go digging through it in the file browser, it is not recommended.\nAll git repositories start with an initialization. For us, that will usually mean starting a new R studio project. It is possible to initialize a git repo in a project that you already have files in, and we will cover that process later in the semester. For now, in the upper right corner of R studio, create a project in a new directory called git_worksheet, and make sure the \u0026ldquo;Create git Repository\u0026rdquo; box is checked when you do.\nOnce the project is created, find the folder in your file browser or finder window, and double click on the \u0026ldquo;git_worksheet.Rproj\u0026rdquo; file to open that R project. In the far upper right of your screen, you should now see that the project is set to git_worksheet.\nYou should also see in the upper right pane a new tab that says git. If you don\u0026rsquo;t, something has gone wrong.\n2. Your First Commit Now that we have a project with a git repository, we can start adding files that we want to keep track of. In general, you want to commit all code files, but not data files. Data files are much larger than what git was made for, and you will quickly run out of storage space on sites like Github if you push them. If you do it by accident, it is possible to break your repo!\nIt is also critically important that you do not commit any files with sensitive information like passwords. Other people will be able to see them, as git is in no way encrypted. It is also very difficult to remove a file from git\u0026rsquo;s memory; it was made to save things!\nNever commit sensitive files like passwords or API keys.\nBefore we go on, a quick review of the git workflow. First, we tell git to start watching the file by staging it. Then, we commit it to git\u0026rsquo;s memory by committing the file. Later in this worksheet, we will then work on pushing files to a remote server online. It is crucial to understand that each of these are separate and distinct steps!\nCreate a new .txt file called git.txt and save it within your project folder. Copy the following into that text file.\niiii tttt i::::i ttt:::t iiii t:::::t t:::::t ggggggggg gggggiiiiiii ttttttt:::::ttttttt g:::::::::ggg::::gi:::::i t:::::::::::::::::t g:::::::::::::::::g i::::i t:::::::::::::::::t g::::::ggggg::::::gg i::::i tttttt:::::::tttttt g:::::g g:::::g i::::i t:::::t g:::::g g:::::g i::::i t:::::t g:::::g g:::::g i::::i t:::::t g::::::g g:::::g i::::i t:::::t tttttt g:::::::ggggg:::::g i::::::i t::::::tttt:::::t g::::::::::::::::g i::::::i tt::::::::::::::t gg::::::::::::::g i::::::i tt:::::::::::tt gggggggg::::::g iiiiiiii ttttttttttt g:::::g gggggg g:::::g g:::::gg gg:::::g g::::::ggg:::::::g gg:::::::::::::g ggg::::::ggg gggggg Look at the git panel in the upper right pane of R Studio. You should now see the git.txt file we just saved. It should have a yellow question marks next to it. That means git is not currently keeping track of that file. It is unknown.\nWe want to tell git to start tracking git.txt. To do so, click on the white check box under the \u0026ldquo;Staged\u0026rdquo; column next to git.txt. Once you click that, the file is staged, meaning when we make our next commit to the git timeline, that file will be saved.\nLet\u0026rsquo;s make out first commit. In the git pane, click on the \u0026ldquo;Commit\u0026rdquo; button. This will open the commit window. Nothing has been committed yet.\nIn this window we will see all our files again on the top left, as well as two new areas. In the top right is a box labeled \u0026ldquo;Commit message.\u0026rdquo; This is where you can write a message that will appear on the git timeline describing what you are adding or changing in this commit. For now, type in \u0026ldquo;adding git art.\u0026rdquo; Press the \u0026ldquo;Commit\u0026rdquo; button once you are done. A progress window will pop up letting you know what is happening. Once it stops changing, you can close it and the commit window.\n3. Making a Change We\u0026rsquo;ve now saved a checkpoint of our git.txt file. This means we can return this file to this exact state at any point in the future. We can even delete it and bring it back from oblivion! For now, we\u0026rsquo;ll just make some changes to it.\nSay this was actually the start of a little report on what git does. Below the git logo, write a three sentence summary of what you know about git thus far. Save the file when you are done.\nNow that we\u0026rsquo;ve saved out file, it should appear in our git panel again with a blue \u0026ldquo;M\u0026rdquo; next to it, signifying the file has been \u0026ldquo;Modified.\u0026rdquo; Repeat the process of staging it (clicking check box), and committing it (going into the commit menu, adding a message, and pressing commit). We have now added another checkpoint to our git timeline.\ngit can only ever see changes to your file after you have saved the file.\n4. Adding More Files Thus far, working with one file seems a lot like saving with extra steps. The true value of git starts to appear once we have multiple files in a project. Create a new R script file called octocat_load.R, fill it with the following, and save it:\n# Code to load in Octocat (github mascot) art octocat = readLines(\u0026quot;https://raw.githubusercontent.com/Adv-R-Programming/Adv-R-Reader/main/content/class_worksheets/02_git/octocat.txt\u0026quot;) writeLines(octocat, \u0026quot;./octocat.txt\u0026quot;) readLines works like read.csv in that it is a function to load data into R. Instead of loading CSVs though, it loads text files. Execute this code and then commit the octocat_load.R, but not the newly created octocat.txt file.\nCreate another new R script called octocat_print.R. Inside this file, copy the following code into it:\n# to load the local octocat data and print it octocat = readLines(\u0026quot;./octocat.txt\u0026quot;) print(octocat) Save this file and commit it.\nCreate one last script called octocat_count.R. In this script, copy the following:\n# to count the lines in octocat.txt octocat = readLines(\u0026quot;./octocat.txt\u0026quot;) length(octocat) Save this file and commit it. We now have a toy example of a fairly common data science workflow:\noctocat_load.R downloads the file from the internet and saves it locally. octocat_print.R is like exploratory data analysis, where we look at the data to understand it. octocat_count.R performs some sort of analysis to convert our data into information. In short, get the data, inspect the data, and perform analyses on the data.\n5. Breaking Things Now that we have out mini data science workflow, we can start to modify it (and break it). Start by opening octocat_load.R. We can replace the readLines function to load the local version of octocat, because we no longer need to grab it from the internet. Replace:\noctocat = readLines(\u0026quot;https://raw.githubusercontent.com/Adv-R-Programming/Adv-R-Reader/main/content/class_worksheets/02_git/octocat.txt\u0026quot;) With\noctocat = readLines(\u0026quot;./octocat.txt\u0026quot;) Save the file and commit the changes.\nSay we want to quickly modify our octocat art by adding an extra line for a caption. Create a new script called octocat_modify.R and add the following code to it:\n# to add a caption to octocat octocat = readLines(\u0026quot;./octocat.txt\u0026quot;) octocat = c(octocat, \u0026quot;ASCII Art of the Octocat Mascot for Github\u0026quot;) writeChar(octocat, \u0026quot;./octocat.txt\u0026quot;) We use c() here to combine octocat with our caption, and then assign it back to our octocat object. Save octocat_modify.R, run all the code, and commit it to the repo.\nGreat, now we have out data updated, let\u0026rsquo;s open up our octocat_print.R and run it again to see our beautiful art again.\nUh-oh. It doesn\u0026rsquo;t work anymore. You may have noticed we used the wrong function to save out modified octocat object (we used writeChar rather than writeLines). That\u0026rsquo;s fine, we can load the data in again in our octocat_load.R script\u0026hellip; but we can\u0026rsquo;t because we changed that script to use the local copy which we just broke.\nTime to power up the time machine.\n6. Time Travel for Beginners In the git pane, click on the \u0026ldquo;History\u0026rdquo; button to open up the git timeline (not the history tab, the history button in the git tab).The history window is broken in to to main parts.\nAt the top you have your git timeline, which shows all of your commits in this project. The timeline shows you commit messages, the author of those commits, when the commit happened, and an \u0026ldquo;SHA\u0026rdquo; which you can think of as a unique ID for that commit. On the bottom is the diff or \u0026ldquo;difference\u0026rdquo; window. It will show you what files were changed in that commit, and how they changes. Sections in green were added, while sections in red were removed.\nFind the commit in the timeline where we changed octocat_load.R. Inside the diff window, on the box labeled octocat_load.R, click on the \u0026ldquo;View file @ ########\u0026rdquo; button in the upper right of the box. This will open the file as it was when you committed it. Use this to fix our octocat_load.R script, and save an working copy of octocat.txt again.\n7. To the Cloud Now we\u0026rsquo;re going to go over how to set our new repo up on Github. Head to https://github.com/ and log in. Once you log in, look to the left side of the screen, and click on the green \u0026ldquo;New\u0026rdquo; button.\nThis will take us to the screen to create a new repo. Enter \u0026ldquo;git_worksheet\u0026rdquo; under Repository name and then scroll to the bottom of the page and click \u0026ldquo;Create repository.\u0026rdquo; You will now see a page called \u0026ldquo;Quick Setup.\u0026rdquo; At the top of this page, make sure you select the \u0026ldquo;SSH\u0026rdquo; option. The HTTPS will work, but later you wil have to use your username and password to log in every time you want to push or pull.\nNext, look at the second box that says \u0026ldquo;\u0026hellip;or push an existing repository from the command line.\u0026rdquo;\nWe are going to use these commands to link our local repo with the one on Github. In R Studio, look at the lower left console pane, and click on the \u0026ldquo;Terminal\u0026rdquo; tab. Enter the three lines of code from Github into the terminal one by one. They should look like this (but use the ones from Github, not these!):\ngit remote add origin git@github.com:\u0026lt;REPO-DETAILS\u0026gt; git branch -M main git push -u origin main Once you have done that, close R Studio and re-open your project (or right click or command click anywhere on R Studio and select \u0026ldquo;Reload\u0026rdquo;). Once you have done that, look at the git pane in the upper right. You will notice you now have the option to click on the \u0026ldquo;Pull\u0026rdquo; and \u0026ldquo;Push\u0026rdquo; buttons. Click \u0026ldquo;Push\u0026rdquo; now, wait for the process to finish, then refresh the page for your new repo on github. You should see your files there!\n8. Conflict So far so good, but sometimes things go awry. On Github, click on the octocat_count.R script to be taken to its page. In the toolbar above the code, on the right hand side, you will see a pencil icon. Click that to edit the file. Add a new comment to the second line that says:\n# Conflict! Scroll to the bottom of the page and click \u0026ldquo;Commit Changes.\u0026rdquo; This simulates as if someone else, or yourself, changed the file somewhere else. We\u0026rsquo;ll get into more detail about collaborating with git later on in the semester.\nNow, in R Studio, open up octocat_count.R and on line 2 there, add a comment that says:\n# It happens. Save the file and commit it using the git pane.\nNow, press the \u0026ldquo;Pull\u0026rdquo; button in the git pane. This time an error will pop up saying you have a conflict. A conflict happens whenever git can\u0026rsquo;t reconcile the differences between two versions of the same file, so it will ask you to resolve the conflict. In R Studio, around line 2 in octocat_count.R, you should now see the following:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD # it happens ======= # Conflict! \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; \u0026lt;NUMBERS AND LETTERS\u0026gt; This is git pointing out where the two versions of the file are different. All the differences will exist between the two rows of arrows, the \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; and \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;. The line of equal signs, =======, separates the two versions.\nTo resolve this, we need to pick which version we want to keep. For now, edit this area, so only the comments that says # Conflict! remains. You are basically creating the \u0026ldquo;canonical\u0026rdquo; version of the file, so it should look like whatever you want all versions of the file to look like. That means you should also delete the \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD, equal signs, and \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; \u0026lt;NUMBERS AND LETTERS\u0026gt;.\nSave the file and commit it again. After you commit the changes, press the \u0026ldquo;Push\u0026rdquo; button to update github.\nConclusion That was an abstracted version of the full git workflow! We\u0026rsquo;ll have lots of time to practice it throughout the semester, so don\u0026rsquo;t feel worried if it didn\u0026rsquo;t feel normal yet. It takes practice.\nWhile it can be a chore, having good git disciple not only can save yourself a lot of heartache, but it makes you a great team member for real data science projects. Who wouldn\u0026rsquo;t want a time wizard who can move back and forth between long lost code versions?\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/install_guide/git/","title":"git","tags":[],"description":"","content":" Overview git on Windows git on Mac Overview git is a tool for version control and collaboration. It is the tool used by data science teams big and small to keep track of code. Think of it like track changes in Word or Google docs, but for code files.\nYou will also need an account on Github. Please create one here.\ngit on Windows Follow these step-by-step instructions if you\u0026rsquo;re installing Git on a Windows machine:\nFirst, launch a web browser, the image below shows the Microsoft Edge browser.\nNext, navigate to the following Git download URL in your browser https://git-scm/com/downloads.\nSelect \u0026ldquo;Windows\u0026rdquo; from the Downloads portion of the Git webpage. Git will display the following page and automatically being downloading the correct version of the Git software. If the download doesn\u0026rsquo;t start automatically, click on the \u0026ldquo;Click here to download manually link.\u0026rdquo;\nWhen the download is complete, open/Run the downloaded file (it may look different in different browsers).\nA screen will appear asking for permissions for the Git application to make changes to your device. Click on the Yes button.\nClick Next to accept the user license.\nLeave the default \u0026ldquo;Destination Location\u0026rdquo; unchanged (usually C:\\Program Files\\Git) and hit Next\nYou will see a screen like the one below asking you to \u0026ldquo;Select Components.\u0026rdquo; Leave all of the default components selected. You can also check the boxes next to \u0026ldquo;Additional Icons\u0026rdquo; and it\u0026rsquo;s sub-item, \u0026ldquo;On the Desktop\u0026rdquo; if you would like. Your completed configurations window should have the following components selected:\nAdditional Icons -\u0026gt; On the Desktop Windows Explorer integration -\u0026gt; Git Bash Here -\u0026gt; Git GUI Here Git LFS (Large File Support) Associate .git* configuration files with default text editor Associate .sh files to be run with Bash The next screen will ask you to pick a \u0026ldquo;default editor, click the drop down box and select\u0026quot;Use the Nano editor by default.\u0026rdquo; The press Next.\nOn the next screen, it will ask to override the default \u0026ldquo;branch name.\u0026rdquo; Select the \u0026ldquo;Override the default branch name for new repositories\u0026rdquo; option, and in the text box type \u0026ldquo;main.\u0026rdquo; Press Next.\nThe next screen will ask you if you want to adjust your path environment. Leave the default of \u0026ldquo;Git from the command line and also from 3rd-party software.\u0026rdquo; Press Next.\nOn the next screen, keep the default option of \u0026ldquo;Use bundled OpenSSH.\u0026rdquo; Press Next\nOn the next screen, keep the default option of \u0026ldquo;Use the OpenSSL library.\u0026rdquo; Press Next.\nLeave the default \u0026ldquo;Checkout Windows-style, commit Unix-style line endings\u0026rdquo; selected on the next page and hit Next:\nKeep the default \u0026ldquo;Use MinTTY (the default terminal of MSYS2)\u0026rdquo; selected on the \u0026ldquo;Configuring the terminal emulator to use with Git Bash\u0026rdquo; window and hit Next:\nKeep the default value of \u0026ldquo;Default (fast-forward or merge)\u0026rdquo; on the \u0026ldquo;Choose the default behavior of \u0026lsquo;git pull\u0026rsquo;\u0026rdquo; page and hit Next:\nKeep the default value of \u0026ldquo;Git Credential Manager Core\u0026rdquo; on the \u0026ldquo;Choose a credential helper\u0026rdquo; page and hit Next:\nKeep the default values on the \u0026ldquo;Configuration extra options\u0026rdquo; page by keeping \u0026ldquo;Enable file system caching\u0026rdquo; checked and \u0026ldquo;Enable symbolic links\u0026rdquo; unchecked and then hit Next:\nMake sure that no options are checked in the \u0026ldquo;Configuring experimental options\u0026rdquo; page and hit Install:\nAfter you hit this Install button as per above, you will see an install progress screen like the one below:\nWhen the install is complete, a new, \u0026ldquo;Completing the Git Setup Wizard\u0026rdquo; window like the one below will appear:\nMake sure that all of the options on this window are unchecked as in the image below and then hit the Finish button:\nThis will complete your installation process. Right click on your desktop, and click on \u0026ldquo;Git Bash here.\u0026rdquo; A black terminal window will open. Type in git --version to check if everything was installed correctly. If you see git version \u0026lt;NUMBERS\u0026gt; you\u0026rsquo;re all set. Now we need to configure some settings.\nClick on the window, and then copy the following and press enter, changing \u0026ldquo;Jane Doe\u0026rdquo; to your name. You must put your name in quotes. git config --global user.name \u0026quot;Jane Doe\u0026quot;\nLastly, copy the following and press enter, changing the email to your email address. git config --global user.email jdoe@example.com\ngit on Mac To install git on a Mac, first open the launchpad by clicking its icon, pressing F4, or by making a pinch motion on the track pad with three fingers and your thumb.\nA terminal window will open up, showing your account name and a symbol, usually $ or #, with a flashing cursor afterwards. You will enter text here to issue commands.\nEnter type in the word git and press enter.\nA window will pop up, asking if you want to install \u0026ldquo;developer tools.\u0026rdquo; Click Install.\nA prompt will appear asking you to agree to the license agreement, click Agree.\nThe software will then start installing. It will take a few minutes to finish. When it is done you will see the following window. Click Done.\nTo make sure everything is installed correctly, go back to the terminal window and enter git --version. You should see a message that says git version \u0026lt;NUMBERS\u0026gt;. If you do, you can move on.\nNext we will need to set up some options. Fist, copy the following into the terminal and press enter to change the default branch name: git config --global init.defaultBranch main\nNext, copy the following and press enter, changing \u0026ldquo;Jane Doe\u0026rdquo; to your name. You must put your name in quotes. git config --global user.name \u0026quot;Jane Doe\u0026quot;\nLastly, copy the following and press enter, changing the email to your email address. git config --global user.email jdoe@example.com\nThanks to the UC Davis DataLab\u0026rsquo;s Install Guide for providing a portion of this guide.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/install_guide/","title":"Install Guides","tags":[],"description":"","content":" [WIN ONLY] Windows Subsystem for Linux Set up a full linux terminal on Windows machines.\nR and R Studio Install and configure R \u0026amp; R Studio.\ngit Learn to install and configure git.\nGithub Learn to create SSH keys and link them with GitHub.\nR Packages Install many of the packages we will use this semester.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/install_guide/github/","title":"Github","tags":[],"description":"","content":" Overview Create an Account Creating SSH Keys and Adding to Github Overview Github is a online code repository that great expands the utility of git. It acts as a clearinghouse for code, and is used worldwide by researchers, government, and industry.\nCreate an Account First up, we need to create an account on github.com. Navigate to the site, and click the Sign up button in the upper right.\nEnter your email and create a password.\nYou will most likely receive an email from Github asking to confirm your account. Go and click that.\nCreating SSH Keys and Adding to Github Creating a new SSH key will invalidate all the places your current SSH key is used! Do not create a new one if you already have one!\nSSH Keys are a way to identify your computer when accessing external resources. Think about it like a password for your computer to log in by itself. The first thing we need to do is create an SSH key for your computer. Open up R Studio, and click on the Terminal tab in the bottom left pane. Copy the following, enter your correct email, and press enter to create a key: ssh-keygen -t ed25519 -C \u0026quot;your_email@example.com\u0026quot;\nYour terminal will look slightly different than my pictures, but the process is the same.\nIt will ask where you want to save the key. Accept the defaults by pressing Enter.\nIt will then ask you to create a pass phrase, press enter twice to skip this step.\nIt will then show a printout of your key, and a little bit of art. I\u0026rsquo;ve greyed mine out here for security.\nNext, type cd and press enter, followed by cat .ssh/id_ed25519.pub. It will print out a code starting with \u0026ldquo;ssh-ed25519 \u0026hellip; your_email@smith.edu.\u0026rdquo; You want to highlight all of that, including the \u0026ldquo;ssh-ed25519\u0026rdquo; and your email, press CTRL or command + C to copy it.\nWe will now return to github.com and login.\nIn the upper right hand corner you will see your user profile dropdown. Click on that and go to Settings.\nOn your setting screen, in the left hand menu, click on \u0026ldquo;SSH and GPG keys.\u0026rdquo;\nOn the next screen, click the green button in the upper right that says \u0026ldquo;New SSH Key.\u0026rdquo;\nOn the following screen, name your key, and paste the text we copied into the \u0026ldquo;Key\u0026rdquo; box. Then press the \u0026ldquo;Add SSH Key\u0026rdquo; button.\nIf you are using multiple terminals (like WSL and git Bash on windows) your SSH keys are specific to each of those programs. So, if you make a key in git Bash, then try to push using WSL, GitHub will not recognize you. You will need to repeat this process for each terminal you intend to use. The same is true of the terminals on remote servers!\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/03_objects/","title":"Objects in R","tags":[],"description":"","content":" Overview Class Attributes Lists Overview Objects in R are the thing we typically work with. We load data into objects, manipulate those objects, and run analyses on them. But for that seamless workflow to happen a surprising amount of work is happening behind the scenes. Today we will be looking behind the curtain a bit to see how R understands objects. We will also be spending some time working with lists.\nFor each of the following questions, write the answer in an R script or Quarto document. The answers are due at the start of the next class day, but will not be turned in. Rather, the answers to these worksheet act as valuable reference for the labs on Friday. Further, if you are not able to answer all of the questions here, you will have a difficult time on the more complex labs. Answers for worksheets will be shared after lecture the day they are due.\nClass The most common way we thing of vectors in R relates to the three big classes: character, numeric, and logical. A vector can only contain one of these types, and R will force that to be the case if it is not already.\nCan you predict what class each of the following vectors will be before running them?\n# example 1 c(TRUE, 8) # example 2 c(\u0026#34;TRUE\u0026#34;, FALSE) # example 3 c(F, F, F, T) # example 4 c(TRUE, TRUE, TRUE, 1) Example 1: Numeric Example 2: Character Example 3: Logical Example 4: Numeric It is possible to force a vector into a specific type though coercion. R will basically take the vector, and do itâs best to present it in the way you ask. You can do this using the as.xxxx() family of functions. For example:\nas.numeric(c(\u0026#34;1\u0026#34;, \u0026#34;8\u0026#34;, \u0026#34;10\u0026#34;)) [1] 1 8 10 class(as.numeric(c(\u0026#34;1\u0026#34;, \u0026#34;8\u0026#34;, \u0026#34;10\u0026#34;))) [1] \u0026quot;numeric\u0026quot; However, this doesnât always work. This coercion is actually the cause of one of the more common warnings you may have seen.\nWhat will happen when I run the following?\nas.numeric(c(\u0026#34;Will\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;work?\u0026#34;)) [1] NA NA NA Warning message: NAs introduced by coercion Words are not numbers, so it fails!\nHere is where we can really start to do odd things given our knowledge of R. the as.xxxx() functions are basically just telling R to look at a vector a different way. If we know what is happening under the hood, we can do it ourselves without the need of the function.\nTry running the following. What happened? Explain the process.\ntest_vec = c(1, 3, 3, 7) class(test_vec) = \u0026#34;character\u0026#34; Just like how we can subset elemetns from a vector or assign to it depending on what side of the equal sign our objects are, we can do the same with object classes. Rather than use an as.xxxx() function to turn this numeric vector into a character, we changed by editing the class ourselves.\nAttributes Attributes are another way R understands objects. You may not have knowingly interacted with them very often, but they are one of the main ways R decides what to do when given an object. There are a few pre-defined attributes, the most common of which being names. The names attribute is what makes named vectors work. For example:\n# make a named vector (weather forecast in degrees F) named_vec = c(\u0026#34;Mon\u0026#34; = 39, \u0026#34;Tues\u0026#34; = 36, \u0026#34;Wed\u0026#34; = 44, \u0026#34;Thur\u0026#34; = 36, \u0026#34;Fri\u0026#34; = 44, \u0026#34;Sat\u0026#34; = 45, \u0026#34;Sun\u0026#34; = 37) named_vec Mon Tues Wed Thur Fri Sat Sun 39 36 44 36 44 45 37 Each of the values in the vector are named, such that when they are shown, the corresponding name will be shown above them. We can still using it like a normal numeric vector in every way though!\n# quick maths named_vec * 2 Mon Tues Wed Thur Fri Sat Sun 78 72 88 72 88 90 74 We can see how these names are stored using the str() or âstructureâ function. You may have used this to preview dataframes before. In this case, we can see the vector, and beneath it is shows all the attributes attached to it.\nstr(named_vec) Named num [1:7] 39 36 44 36 44 45 37 - attr(*, \u0026quot;names\u0026quot;)= chr [1:7] \u0026quot;Mon\u0026quot; \u0026quot;Tues\u0026quot; \u0026quot;Wed\u0026quot; \u0026quot;Thur\u0026quot; ... names is a special attribute that R recognizes, but you can create your own custom attributes too.\nUsing the attr() function, add a new attribute to the named_vec from above.\nattr(named_vec, âhappy_scoreâ) = c(1, 1, 3, 3, 4, 5, 2)\nLists Thus far we have been working with vectors, which are commonly used as components of larger data structures. The one you are probably most familiar with is dataframes. Take for example the following:\n# get the built in mtcars data cars = mtcars # subset suing $ mtcars$mpg [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 [31] 15.0 21.4 # subset using [] mtcars[, \u0026#34;mpg\u0026#34;] [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 [31] 15.0 21.4 Both of these are ways to subset out one column of a dataframe, which is a vector.\nWorking with a shallow list, one with only one layer of depth, is much the same. Take for example the list below. It contains three elements: 1) a numeric vector, 2) a character vector, and 3) a dataframe.\n# make an example list to work with example_list = list(\u0026#34;numbers\u0026#34; = c(1, 2, 5, 7, 3, 5), \u0026#34;letters\u0026#34; = c(\u0026#34;y\u0026#34;, \u0026#34;u\u0026#34;, \u0026#34;n\u0026#34;, \u0026#34;r\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;b\u0026#34;), \u0026#34;df\u0026#34; = mtcars) When trying to subset elements of a list, we can similarly use both $ and []. However, there is one major difference, and that is in how we use the brackets []. When using the brackets on a list, one set of brackets (list_name[]) will get us the element of the list we request, while two sets (list_name[[]]) will get the contents of that element. See for example the differences below:\n# get element 1 example_list[1] $numbers [1] 1 2 5 7 3 5 # get contents of element 1 example_list[[1]] [1] 1 2 5 7 3 5 The first call returns a list of length 1, which contains the numbers vector, while the second actually returns the numbers vector itself. Forgetting this will cause you a lot of grief later on!\nWith that importance difference in mind, we can use any of the following strategies to get the numbers vector out of our list.\n# using $ example_list$numbers [1] 1 2 5 7 3 5 # using name example_list[[\u0026#34;numbers\u0026#34;]] [1] 1 2 5 7 3 5 # using position example_list[[1]] [1] 1 2 5 7 3 5 Now that we can get out a vector, we can subset as normal from there.\nFrom the example list we just made, subset the following:\nThe number 7 from the number vector The letters âyâ and âtâ from the character vector This entire mtcars dataframe example_list$numbers[4] / example_list[[\u0026ldquo;numbers]][4] / example_list$numbers[4] example_list$letters[c(1, 5)] / example_list[[âlettersâ]][c(1, 5)] / example_list[[2]][c(1, 5)] example_list$df / example_list[[âdfâ]] / example_list[[3]] "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/","title":"Worksheets","tags":[],"description":"","content":" git Practise some git fundamentals.\nObjects in R Learn the many ways R sees objects.\nFunctions Fundamentals of functions.\nDebugging \u0026amp; Flow Tools to fix errors and test conditions.\nIteration Iterate through elements and give our function superpowers.\nList and Apply Learn the apply family of functions.\nWeb Scraping Harvest the internet.\nRegEx Learn some RegEx interactively!\nParallel Learn to parallelize R code.\nPDF Data Extraction Extract data from PDFs.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/labs/","title":"Labs","tags":[],"description":"","content":"All labs will be turned in through Github Classroom. You will set up your student account in the process on completing your first lab. Click the link below to open the lab.\nLab 01: R Objects Lab 02: Functions Lab 03: Iteration \u0026amp; Apply Lab 04: Web Scraping \u0026amp; RegEx Lab 05: PDF \u0026amp; Parallel "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/install_guide/r_packages/","title":"R Packages","tags":[],"description":"","content":" Overview Overview R uses a number of packages to work with data, which are largely community created. This means many of them do not come pre-installed with R. Here is a list of packages we will use this semester. You should be able to paste this into the R console and press enter to install them all at once.\ninstall.packages(\u0026quot;tidyverse\u0026quot;); install.packages(\u0026quot;devtools\u0026quot;); install.packages(\u0026quot;dplyr\u0026quot;); install.packages(\u0026quot;todor\u0026quot;); install.packages(\u0026quot;future\u0026quot;); install.packages(\u0026quot;rvest\u0026quot;); install.packages(\u0026quot;RSQLite\u0026quot;) "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/05_functions/","title":"Functions","tags":[],"description":"","content":" Overview The Data Functions Under the Hood Solving a Problem Make it General Try it Yourself Overview Functions are the backbone of how R does things; we have refereed to them previously as âverbsâ in the language of R. We have used several functions so far, either from base R, or from packages we have loaded in using library(). However, those are not all the functions available to us. The true power in R comes from our ability to make our own functions to do anything we want. Thatâs what weâll be practicing today.\nThe Data We are going to be using class survey data for lab today. Please load it in using the following:\nsurvey = readRDS(url(\u0026#34;https://github.com/Adv-R-Programming/Adv-R-Reader/raw/main/class_survey.rds\u0026#34;)) Functions Under the Hood We can peek behind the curtain on any of the functions we have used so far an see how they tick. All you have to do is enter the function into the console, and remove the () where the arguments go. Letâs look at our friend table() by typing table into the console and hitting enter. You should see the following (click the green âCodeâ fold button to see):\nCode function (..., exclude = if (useNA == \u0026#34;no\u0026#34;) c(NA, NaN), useNA = c(\u0026#34;no\u0026#34;, \u0026#34;ifany\u0026#34;, \u0026#34;always\u0026#34;), dnn = list.names(...), deparse.level = 1) { list.names \u0026lt;- function(...) { l \u0026lt;- as.list(substitute(list(...)))[-1L] if (length(l) == 1L \u0026amp;\u0026amp; is.list(..1) \u0026amp;\u0026amp; !is.null(nm \u0026lt;- names(..1))) return(nm) nm \u0026lt;- names(l) fixup \u0026lt;- if (is.null(nm)) seq_along(l) else nm == \u0026#34;\u0026#34; dep \u0026lt;- vapply(l[fixup], function(x) switch(deparse.level + 1, \u0026#34;\u0026#34;, if (is.symbol(x)) as.character(x) else \u0026#34;\u0026#34;, deparse(x, nlines = 1)[1L]), \u0026#34;\u0026#34;) if (is.null(nm)) dep else { nm[fixup] \u0026lt;- dep nm } } miss.use \u0026lt;- missing(useNA) miss.exc \u0026lt;- missing(exclude) useNA \u0026lt;- if (miss.use \u0026amp;\u0026amp; !miss.exc \u0026amp;\u0026amp; !match(NA, exclude, nomatch = 0L)) \u0026#34;ifany\u0026#34; else match.arg(useNA) doNA \u0026lt;- useNA != \u0026#34;no\u0026#34; if (!miss.use \u0026amp;\u0026amp; !miss.exc \u0026amp;\u0026amp; doNA \u0026amp;\u0026amp; match(NA, exclude, nomatch = 0L)) warning(\u0026#34;\u0026#39;exclude\u0026#39; containing NA and \u0026#39;useNA\u0026#39; != \\\u0026#34;no\\\u0026#34;\u0026#39; are a bit contradicting\u0026#34;) args \u0026lt;- list(...) if (length(args) == 1L \u0026amp;\u0026amp; is.list(args[[1L]])) { args \u0026lt;- args[[1L]] if (length(dnn) != length(args)) dnn \u0026lt;- paste(dnn[1L], seq_along(args), sep = \u0026#34;.\u0026#34;) } if (!length(args)) stop(\u0026#34;nothing to tabulate\u0026#34;) bin \u0026lt;- 0L lens \u0026lt;- NULL dims \u0026lt;- integer() pd \u0026lt;- 1L dn \u0026lt;- NULL for (a in args) { if (is.null(lens)) lens \u0026lt;- length(a) else if (length(a) != lens) stop(\u0026#34;all arguments must have the same length\u0026#34;) fact.a \u0026lt;- is.factor(a) if (doNA) aNA \u0026lt;- anyNA(a) if (!fact.a) { a0 \u0026lt;- a op \u0026lt;- options(warn = 2) a \u0026lt;- factor(a, exclude = exclude) options(op) } add.na \u0026lt;- doNA if (add.na) { ifany \u0026lt;- (useNA == \u0026#34;ifany\u0026#34;) anNAc \u0026lt;- anyNA(a) add.na \u0026lt;- if (!ifany || anNAc) { ll \u0026lt;- levels(a) if (add.ll \u0026lt;- !anyNA(ll)) { ll \u0026lt;- c(ll, NA) TRUE } else if (!ifany \u0026amp;\u0026amp; !anNAc) FALSE else TRUE } else FALSE } if (add.na) a \u0026lt;- factor(a, levels = ll, exclude = NULL) else ll \u0026lt;- levels(a) a \u0026lt;- as.integer(a) if (fact.a \u0026amp;\u0026amp; !miss.exc) { ll \u0026lt;- ll[keep \u0026lt;- which(match(ll, exclude, nomatch = 0L) == 0L)] a \u0026lt;- match(a, keep) } else if (!fact.a \u0026amp;\u0026amp; add.na) { if (ifany \u0026amp;\u0026amp; !aNA \u0026amp;\u0026amp; add.ll) { ll \u0026lt;- ll[!is.na(ll)] is.na(a) \u0026lt;- match(a0, c(exclude, NA), nomatch = 0L) \u0026gt; 0L } else { is.na(a) \u0026lt;- match(a0, exclude, nomatch = 0L) \u0026gt; 0L } } nl \u0026lt;- length(ll) dims \u0026lt;- c(dims, nl) if (prod(dims) \u0026gt; .Machine$integer.max) stop(\u0026#34;attempt to make a table with \u0026gt;= 2^31 elements\u0026#34;) dn \u0026lt;- c(dn, list(ll)) bin \u0026lt;- bin + pd * (a - 1L) pd \u0026lt;- pd * nl } names(dn) \u0026lt;- dnn bin \u0026lt;- bin[!is.na(bin)] if (length(bin)) bin \u0026lt;- bin + 1L y \u0026lt;- array(tabulate(bin, pd), dims, dimnames = dn) class(y) \u0026lt;- \u0026#34;table\u0026#34; y } Thatâs pretty complex looking; and it is. However, the point is that this is the same code that runs when we call table. We could, it we wanted, type this out ourselves and create our own table function using the function() function.\nCopy the code from the table() function I showed above, and create a new function called my_table() using that code. Run both table() and my_table() on the mint_choc column from our survey data. Does the output make sense?\nmy_table = table\ntable(survey$mint_choc) my_table(survey$mint_choc)\nNot all functions are as transparent. Try looking at the code behind the sum() function and you may be disappointed. The most basic functions in R are âprimitives,â and are actually calling a lower level programming language (C) than we can access through R. Looking at sum we see:\nfunction (..., na.rm = FALSE) .Primitive(\u0026#34;sum\u0026#34;) You may also be surprised to learn that pretty much everything that isnât data in R is a function. For example, we can look at the + sign as a function by warping it in ticks like so:\n`+` function (e1, e2) .Primitive(\u0026quot;+\u0026quot;) Thus, we could also call:\n`+`(1, 5) [1] 6 Neat.\nSolving a Problem Itâs cool that we can make a copy of table, but why would we? We wouldnât really. What we would do is create our own functions. While there a ton of packages out there to do all sorts of things, sometimes you will just need to make something yourself. This is often the case when working with your own unique data sets.\nLetâs take a look at our survey data, specifically the pets column. As you can see below, we have some issues that violate tidy data principles. Specifically, in our one column of pets, we have values on multiple types of pets. Ideally, we would like a column for each type of pet, and then a TRUE or FALSE if people had that kind of pet.\nsurvey$pets [1] \u0026quot;Cat\u0026quot; \u0026quot;None\u0026quot; [3] \u0026quot;Dog\u0026quot; \u0026quot;None\u0026quot; [5] \u0026quot;None\u0026quot; \u0026quot;Dog, Cat, Rock\u0026quot; [7] \u0026quot;Bird\u0026quot; \u0026quot;Dog\u0026quot; [9] \u0026quot;None\u0026quot; \u0026quot;Cat\u0026quot; [11] \u0026quot;None\u0026quot; \u0026quot;Bird\u0026quot; [13] \u0026quot;None\u0026quot; \u0026quot;Dog, Cat\u0026quot; [15] \u0026quot;Dog, Bird\u0026quot; \u0026quot;Bird\u0026quot; [17] \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; [19] \u0026quot;Reptile\u0026quot; \u0026quot;Rock, None\u0026quot; [21] \u0026quot;None\u0026quot; \u0026quot;Cat\u0026quot; [23] \u0026quot;None\u0026quot; \u0026quot;Robot Vacuum named Tobie\u0026quot; Hereâs where a custom function can help us. We know that each of the entries in pets is separated by a comma, and we can exploit that structure. Weâre going to write a function that will split those values for us by commas, and create a new pets dataframe that we can then attach to our current survey dataframe.\nThe first step of creating a good function is clearly defining the key components: the input, output, and arguments. In our case, the input will be our pets column, the output will be a dataframe with one column per pet type, and a value of TRUE or FALSE depending on if the case had that pet. We wonât need any arguments aside from the input for now.\nWhen I am creating a function, I typically write the code to do what I want first, and then convert it into a function. Letâs write some code to create our output dataframe, and then we can work to fill it. I want to stress that this is one way to go about solving this problem. There are plenty of other valid (and easier, once we learn some more skills) options.\nFirst, weâll create a new dataframe with a column for each of our possible pets, and a row for all 15 of our cases. Iâll fill the dataframe with NAs for now.\npet_output = data.frame( \u0026#34;id\u0026#34; = 1:nrow(survey), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) Now, we need to figure out a way to test if the person listed one of our options in their response. Weâll use a new function called grepl() to test this. The name âgreplâ is a product of very old programmer speak, but practically it will search through a character vector and give us a TRUE or FALSE if it finds a match for the pattern we give it. Thus, we can run the following on our pets column for each of our possible pets to test if they are included. Weâre going to tell it to ignore case, so that capital and lower case letters donât matter.\nRun the following code and explain itâs output.\ngrepl(pattern = \u0026#34;dog\u0026#34;, x = survey$pets, ignore.case = TRUE) It says TRUE if the value included âdogâ and FALSE if it did not.\nWe can use the same function to test for each of our possible pets, and assign the results to our new dataframe.\npet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output id dog cat fish bird reptile rock none other 1 1 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 5 5 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 6 6 TRUE TRUE FALSE FALSE FALSE TRUE FALSE NA 7 7 FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 9 9 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 11 11 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 12 12 FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA 13 13 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 14 14 TRUE TRUE FALSE FALSE FALSE FALSE FALSE NA 15 15 TRUE FALSE FALSE TRUE FALSE FALSE FALSE NA 16 16 FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA 17 17 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 18 18 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 19 19 FALSE FALSE FALSE FALSE TRUE FALSE FALSE NA 20 20 FALSE FALSE FALSE FALSE FALSE TRUE TRUE NA 21 21 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 22 22 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 23 23 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 24 24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE NA Now, that works for everything except âother.â\nWhy would we need a different strategy for the âotherâ cases?\nWe donât know what will be in âotherâ so we canât test explicitly for it.\nTo resolve our âotherâ issue, we will need to take another approach. First, weâll make a copy of our pets column we can work with, and make sure we donât alter our original data.\nwork_pets = survey$pets Next, we will look through that copy, and remove everything that fits into one of our other categories using gsub(), or âgeneral substitution.â gsub() is similar to grepl() in that it asks for a pattern to look for and where to look for it, but also asks what to substitute for that pattern. Letâs look at the following example. Here I ask gsub() to look in our work_pets vector, find all the times âdogâ appears, and to replace it with ââ, or nothing.\ngsub(pattern = \u0026#34;dog\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) [1] \u0026quot;Cat\u0026quot; \u0026quot;None\u0026quot; [3] \u0026quot;\u0026quot; \u0026quot;None\u0026quot; [5] \u0026quot;None\u0026quot; \u0026quot;, Cat, Rock\u0026quot; [7] \u0026quot;Bird\u0026quot; \u0026quot;\u0026quot; [9] \u0026quot;None\u0026quot; \u0026quot;Cat\u0026quot; [11] \u0026quot;None\u0026quot; \u0026quot;Bird\u0026quot; [13] \u0026quot;None\u0026quot; \u0026quot;, Cat\u0026quot; [15] \u0026quot;, Bird\u0026quot; \u0026quot;Bird\u0026quot; [17] \u0026quot;None\u0026quot; \u0026quot;None\u0026quot; [19] \u0026quot;Reptile\u0026quot; \u0026quot;Rock, None\u0026quot; [21] \u0026quot;None\u0026quot; \u0026quot;Cat\u0026quot; [23] \u0026quot;None\u0026quot; \u0026quot;Robot Vacuum named Tobie\u0026quot; We can see in the output that now all the instances of âdogâ have been removed. We will repeat this process for each of our known categories, each time saving our results back to work_pets. We will also remove commas, and use trimws(), or âtrim white space,â to delete extra spaces from the start and end of our characters.\nwork_pets = gsub(pattern = \u0026#34;dog\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;cat\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;fish\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;bird\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;reptile\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;rock\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;none\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;,\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = trimws(work_pets) work_pets [1] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [3] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [5] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [7] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [9] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [11] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [13] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [15] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [17] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [19] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [21] \u0026quot;\u0026quot; \u0026quot;\u0026quot; [23] \u0026quot;\u0026quot; \u0026quot;Robot Vacuum named Tobie\u0026quot; If we look at work_pets now, all that is left are things not in our pets categories. We can now either turn this into a logical, or assign these values to our new other column in pet_output. Iâll do the latter. Iâll also convert our blanks into proper NAs.\npet_output$other = work_pets pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA pet_output id dog cat fish bird reptile rock none other 1 1 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 6 6 TRUE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 12 12 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 14 14 TRUE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 15 15 TRUE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 16 16 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 17 17 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 18 18 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 19 19 FALSE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 20 20 FALSE FALSE FALSE FALSE FALSE TRUE TRUE \u0026lt;NA\u0026gt; 21 21 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 22 22 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 23 23 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 24 24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Robot Vacuum named Tobie If we look at out pet_output dataframe now, we can see we have a column for each pet type, a TRUE or FALSE for each known pet, and the text for other pets. All in all, the code to do this looks like:\n# create an empty dataframe for our intended output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:nrow(survey), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = survey$pets, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = survey$pets, ignore.case = TRUE) # make a copy of the pets vector to work on work_pets = survey$pets # remove all known pets and clean remaining text work_pets = gsub(pattern = \u0026#34;dog\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;cat\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;fish\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;bird\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;reptile\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;rock\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;none\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = gsub(pattern = \u0026#34;,\u0026#34;, work_pets, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) work_pets = trimws(work_pets) # Fill in \u0026#34;other\u0026#34; pet_output$other = work_pets # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA Make it General Now that we know what to do, letâs get to work converting this into a function. The key advantage of doing so is that we can make out code more generalizable. Rather than hard-coding each element, we can instead use variables for stand-ins that can be swapped later on. For example, instead of performing all of these checks on survey$pets, we can write the function to look at any pets vector we pass as an argument. This means we can re-use the function later!\nTo start this process, letâs write a skeleton for our function. Below Iâve included a skeleton for a new function I am calling pet_split. In this function, I have an argument called pet_vector. Now, whenever I use pet_vector inside the body of the function, I will be telling R to use whatever is given to the function in the pet_vector argument. pet_vector is just like an ordinary object in your R environment, except it only exists within the function. You can think about it like R opening a little mini-R, running all the code in the function top to bottom, giving you the result, then closing it and deleting everything else inside.\nIâll say it again because it is so important to understand. You can think about functions as if they were opening a little mini-R universe, running all the code in the function top to bottom with the arguments you provided as objects, giving you the result, then destroying that universe and deleting everything else inside.\npet_split = function(pet_vector) { } Now letâs add some substance to this function. Iâll start by adding in the code to create a dataframe for our output, and a return() at the end. Whatever I put inside return() will be the result of the function when it is run, and everything else will be deleted when the mini-R inside the function is closed. I also changed the code of our âidâ column a bit. Instead of hard-coding 15 IDs, I made it more generalizable by asking R to make IDs 1 through the number of elements, or the length, of our pet_vector argument. That means the dataframe will always have the same number of rows as the pet_vector input, no matter how many elements it has.\nThe only thing that will come out of your function is whatever you put in the return() function. Messages (like print()) are exceptions.\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 NA NA NA NA NA NA NA NA 2 2 NA NA NA NA NA NA NA NA 3 3 NA NA NA NA NA NA NA NA 4 4 NA NA NA NA NA NA NA NA 5 5 NA NA NA NA NA NA NA NA 6 6 NA NA NA NA NA NA NA NA 7 7 NA NA NA NA NA NA NA NA 8 8 NA NA NA NA NA NA NA NA 9 9 NA NA NA NA NA NA NA NA 10 10 NA NA NA NA NA NA NA NA 11 11 NA NA NA NA NA NA NA NA 12 12 NA NA NA NA NA NA NA NA 13 13 NA NA NA NA NA NA NA NA 14 14 NA NA NA NA NA NA NA NA 15 15 NA NA NA NA NA NA NA NA 16 16 NA NA NA NA NA NA NA NA 17 17 NA NA NA NA NA NA NA NA 18 18 NA NA NA NA NA NA NA NA 19 19 NA NA NA NA NA NA NA NA 20 20 NA NA NA NA NA NA NA NA 21 21 NA NA NA NA NA NA NA NA 22 22 NA NA NA NA NA NA NA NA 23 23 NA NA NA NA NA NA NA NA 24 24 NA NA NA NA NA NA NA NA Next, letâs get this function to output something we actually want. Iâll copy more of our code from above into the function. Youâll notice that I replace any calls for survey$pets with our generic argument pet_vector. If we run this version, we start to see our desired output!\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 5 5 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 6 6 TRUE TRUE FALSE FALSE FALSE TRUE FALSE NA 7 7 FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE NA 9 9 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 11 11 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 12 12 FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA 13 13 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 14 14 TRUE TRUE FALSE FALSE FALSE FALSE FALSE NA 15 15 TRUE FALSE FALSE TRUE FALSE FALSE FALSE NA 16 16 FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA 17 17 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 18 18 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 19 19 FALSE FALSE FALSE FALSE TRUE FALSE FALSE NA 20 20 FALSE FALSE FALSE FALSE FALSE TRUE TRUE NA 21 21 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 22 22 FALSE TRUE FALSE FALSE FALSE FALSE FALSE NA 23 23 FALSE FALSE FALSE FALSE FALSE FALSE TRUE NA 24 24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE NA To finish it up, letâs copy the rest of our code into this function. Youâll notice that I donât need to make a new vector work_pets as before. Thatâs because Iâm working on the pet_vector object directly.\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # remove all known pets and clean remaining text pet_vector = gsub(pattern = \u0026#34;dog\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;cat\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;fish\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;bird\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;reptile\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;rock\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;none\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;,\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#34;other\u0026#34; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 6 6 TRUE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 12 12 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 14 14 TRUE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 15 15 TRUE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 16 16 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 17 17 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 18 18 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 19 19 FALSE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 20 20 FALSE FALSE FALSE FALSE FALSE TRUE TRUE \u0026lt;NA\u0026gt; 21 21 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 22 22 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 23 23 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 24 24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Robot Vacuum named Tobie Our new pet_split function works! It will create a new dataframe we can combine with our survey data to get tidy data about pets. Now we could save this function somewhere, and re-use it on every survey with a question about pets without having to re-code all of those steps each time.\nTake some time to study the above function. Make sure you understand all of the steps it is taking, and how it produces the output that it does.\nNow, we could make this even more general. In reality here we are solving the problem of splitting values by commas; several of our columns have that problem! We will go over how to make this function even better next week when we learn about iteration.\nTry it Yourself Letâs try creating a function on your own now.\nCreate a function that will intake a ROW from the survey dataframe, and output a number showing how many total NAs there were in there row. The is.na() function will come in handy here. An example input and output should look like:\ntotal_na(survey_row = survey[1,]) OUTPUT: 4 total_na = function(survey_row) {\n# get the sum of NAs output = sum(is.na(survey_row))\n# output return(output) }\ntotal_na(survey_row = survey[1,])\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/slides/","title":"Slides","tags":[],"description":"","content":" Lecture Slides 02_1_git.html (6 MB) 02_2_objects.html (6 MB) 03_1_functions.html (3 MB) 03_2_debugging.html (9 MB) 04_1_iteration.html (3 MB) 04_2_list_apply.html (4 MB) 05_1_web_scrape.html (6 MB) 05_2_regex.html (3 MB) 06_1_parallel.html (7 MB) 06_2_pdf_data.html (5 MB) 07_1_bash.html (6 MB) "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/06_debugging/","title":"Debugging &amp; Flow","tags":[],"description":"","content":" Overview Our Toolbox Debugging Example Puzzle 1 Puzzle 2 Overview Code will hardly ever work exactly as you want the first try. Especially early on, coding is an exercise in incremental improvements. Debugging, or identifying and removing âbuggyâ code that doesnât work as intended, is the skill that lets us identify what is wrong so we can make those improvements.\nTodayâs worksheet is presented as a series of puzzles. Each puzzle will be a function that has something wrong with it. I will provide an input, and the desired output. Your task is to use the debugging tools we learned to figure out what is wrong with the function, and correct it. I will walk through an example first so you get the idea.\nWe will be using our class survey data today, so run the following to load it into your environment:\nsurvey = readRDS(url(\u0026#34;https://github.com/Adv-R-Programming/Adv-R-Reader/raw/main/class_survey.rds\u0026#34;)) Our Toolbox We will be using two main tools for debugging, debugonce() and browser(). Each accomplishes the same thing is slightly different ways. These functions let you pause the execution of code mid-way inside a function, and see what is going inside our mini-R universes. This is very helpful, as opposed to just running code in your global environment, you canât normally run code inside a function line-by-line to see what is happening to the data at each step. debugonce() and browser() let you do that. This is also very helpful while building new functions.\ndebugonce() accepts a function name, and the next time you run a function, it will drop you into the mini-universe of that function for you to look around. You can tell it worked because your console will change slightly.\nThe figure above shows what the browser window will look like. While in the browser, you can execute R code like normal, but there are a few differences.\nInstead of the regular \u0026gt; prompt in the R console, you will see Browse[#]\u0026gt; indicating you are in the browser. It still works mostly like the normal console, with a few extra commands. a) You can press Enter or enter the letter n to go to the next line of code. b) You can enter c to continue to the end of the function c) You can enter q to quit and leave the browser The script pane is replaced with a function inspector walking you through the function you are debugging. You normally canât type in this window. The current line, what will be executed next time you press enter or n is highlighted. A few new buttons show up. These are the same as the commands described in #1. We can also use the browser() function to call the browser at a specific spot within a function. Simply add the browser() function anywhere inside a function you are writing and define the function again by executing it. Now, whenever you run that function, the browser will open wherever you added browser(). You will have to remove it from your function once you finish debugging.\nDebugging Example Here is an example function that needs some debugging. This one is relatively short, and you may be able to figure out the problem without debugging. This will not always be the case, as functions will routinely extend for a dozen or several dozen lines with multiple other function inside of them creating nested mini-universes. The debugging process will always be the same though: figure out what function the problem is in, then go inside and follow the process step-by-step.\nThis function is meant to accept a numeric vector, and then output the mean, median, and mode. Instead, it results in the error shown below.\nexample_vector = c(1, 2, 6, 8, 4, 2, 8, 2, 7, 10, 33) example_function = function(num_vec) { # get the mean vec_mean = mean(num_vec) # get the median vec_median = median(num_vec) # get the mode vec_mode = mode(num_vec) # create named vector for output output = c(\u0026#34;mean\u0026#34; = vec_mean, \u0026#34;median\u0026#34; = vec_median, \u0026#34;mode\u0026#34; = vec_mode) # make sure all results are numeric if(!all(is.numeric(output))){stop(\u0026#34;Not all values are numeric!\u0026#34;)} # return results return(output) } example_function(example_vector) Error in example_function(example_vector): Not all values are numeric! How would we go about fixing this? We only have one function, so we know where things must be going wrong. Weâll use debugonce() to get a peak inside. Copy the above function code into your console and execute it to add the function to your environment. Run example_function() on example_vector to make sure you are getting the same output as we did here.\nOnce you have done that, run debugonce(example_function), then run example_function(example_vector) again. You will be dropped into the browser, looking around inside example_function(). Step through the code execution one line at a time by pressing the Enter key. Watch the environment pane each step of the way and see if you can catch where the error will happen. Once you get in the spot in the function the error occurs, it will boot you out of the browser back to the global environment.\nAs you step through the function you should notice the code at line vec_mode = mode(num_vec) produces an output of \u0026quot;numeric\u0026quot;, which would be causing our error in the next line, if(!all(is.numeric(output))){stop(\u0026quot;Not all values are numeric!\u0026quot;)}. That code asking, if all output is not (because !) numeric, then run stop().\nWe could use browser() to check that section more quickly using the following:\nexample_vector = c(1, 2, 6, 8, 4, 2, 8, 2, 7, 10, 33) example_function = function(num_vec) { # get the mean vec_mean = mean(num_vec) # get the median vec_median = median(num_vec) # -------------------------------------------------------------Browser will stop execution here. browser() # get the mode vec_mode = mode(num_vec) # create named vector for output output = c(\u0026#34;mean\u0026#34; = vec_mean, \u0026#34;median\u0026#34; = vec_median, \u0026#34;mode\u0026#34; = vec_mode) # make sure all results are numeric if(!all(is.numeric(output))){stop(\u0026#34;Not all values are numeric!\u0026#34;)} # return results return(output) } example_function(example_vector) If you re-define our example_function() using the code above then try to use it, it will always stop at the browser() function to let us look around. Try it out yourself!\nPuzzle 1 The following function will intake a vector of character names, and output a dataframe with 6 columns. The function will have each character flip a coin. If they get a heads, they can flip again, up to a max of three. If a character flips heads three times, the lucky column should be set to TRUE. Run the following several times. Every so often, a character will appear where they did not flip heads all three times, but get a TRUE in the lucky column. Fix this error.\n# get our class favorite characters char_vec = survey$fav_char puzzle_1 = function(characters) { # sort chars by alphabetical order sorted_char = sort(char_vec) # get the first letter of each name char_letters = substr(x = sorted_char, start = 1, stop = 1) # create a dataframe of character and their initial char_df = data.frame(\u0026#34;char_name\u0026#34; = sorted_char, \u0026#34;char_initial\u0026#34; = char_letters) # randomly flip a count for each char char_df$toss_1 = sample(x = c(\u0026#34;heads\u0026#34;, \u0026#34;tails\u0026#34;), size = nrow(char_df), replace = TRUE) # for each that got heads, flip again, those with tails are out char_df$toss_2 = ifelse(char_df$toss_1 == \u0026#34;heads\u0026#34;, sample(x = c(\u0026#34;heads\u0026#34;, \u0026#34;tails\u0026#34;), size = nrow(char_df), replace = TRUE), NA) # do it again char_df$toss_3 = ifelse(char_df$toss_1 == \u0026#34;heads\u0026#34;, sample(x = c(\u0026#34;heads\u0026#34;, \u0026#34;tails\u0026#34;), size = nrow(char_df), replace = TRUE), NA) # add TRUE / FALSE for those with 3 heads ## set to TRUE if the 3rd toss is heads ## (as the other two had to be heads to toss a third time) char_df$lucky = ifelse(char_df$toss_3 == \u0026#34;heads\u0026#34;, TRUE, FALSE) ## fill NAs with FALSE char_df[is.na(char_df$lucky), \u0026#34;lucky\u0026#34;] = FALSE # return results return(char_df) } puzzle_1(char_vec) Fix the line:\n# do it again char_df$toss_3 = ifelse(char_df$toss_1 == 'heads', sample(x = c('heads', 'tails'), size = nrow(char_df), replace = TRUE), NA) So that it looks at toss_2 rather than toss_1.\nPuzzle 2 The following function will input our survey dataframe, and is meant to output the number of times people responded TRUE to a question. The correct output is 41, however it is currently outputting 112. Debug this function to fix the issue.\npuzzle_2 = function(survey_dataframe) { # pivot the survey data from wide to long survey_long = tidyr::pivot_longer(survey_dataframe, cols = -fav_char, values_transform = as.character) # get all the questions people answered TRUE all_true = survey_long[survey_long$value == TRUE, ] # count the number of rows (the number of questions with answers of TRUE) num_true = nrow(all_true) # return that number return(num_true) } puzzle_2(survey) When we subset using:\nall_true = survey_long[survey_long$value == TRUE, ] It also includes all NAs. You can either account for the NAs like this:\nall_true = survey_long[survey_long$value == TRUE \u0026amp; !is.na(survey_long$value), ] Or subset the dataframe again like:\nall_true = survey_long[survey_long$value == TRUE, ] all_true = all_true[!is.na(all_true$value), ] "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/08_iteration/","title":"Iteration","tags":[],"description":"","content":" Overview The Data A Refresher on our pet_split() Function Break it Down Arbitrary Output Test for Each Option Remove the Known Options to Find âOtherâ Turn it Back into a Function Overview Iteration allows us to tell R to work on a whole sets of things at once: multiple files, multiple columns, multiple whatever. This can save quite a bit of time. It also lets us to work on problems with dependence, where the decisions of each step depends on the result of the previous step.\nFor our worksheet today, we are going to be solving some annoyances of the past. I am going to walk you though modifying our pet_split() function from last weekâs functions worksheet to make it even more generalizable.\nThe Data We are going to be using class survey data for lab today. Please load it in using the following:\nsurvey = readRDS(url(\u0026#34;https://github.com/Adv-R-Programming/Adv-R-Reader/raw/main/class_survey.rds\u0026#34;)) A Refresher on our pet_split() Function Recall from last week that our pet_split() function looked at the pets column in our survey dataframe, and tidy-ed up the column so that instead of having a single character with multiple pets in it, we had a dataframe with TRUE and FALSE for each pet type, along with âother.â You can see the finished function below:\npet_split = function(pet_vector) { # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # remove all known pets and clean remaining text pet_vector = gsub(pattern = \u0026#34;dog\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;cat\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;fish\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;bird\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;reptile\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;rock\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;none\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;,\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#34;other\u0026#34; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA # return return(pet_output) } pet_split(pet_vector = survey$pets) id dog cat fish bird reptile rock none other 1 1 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 3 3 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 4 4 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 6 6 TRUE TRUE FALSE FALSE FALSE TRUE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 8 8 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 10 10 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 12 12 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 14 14 TRUE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 15 15 TRUE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 16 16 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 17 17 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 18 18 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 19 19 FALSE FALSE FALSE FALSE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 20 20 FALSE FALSE FALSE FALSE FALSE TRUE TRUE \u0026lt;NA\u0026gt; 21 21 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 22 22 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 23 23 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 24 24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE Robot Vacuum named Tobie Now, that is cool, but we can make it better. Specifically, if we look at our survey dataframe, we have the exact same problem in our \u0026lt;DRINK\u0026gt;_days columns and the recreation column. By the end of this worksheet, our pet_split() function will work on any column with comma separated values.\nBreak it Down Our first step is going to be writing the code to accomplish what we want, then we can package it as a function. Iâve gutted our pet_split() function below. We will be starting from that base and working to make it so that we never call for anything specific to pets in our code. For example, instead of coding all of the possibilities of pet (dog, cat, fish, bird, reptile, rock, none) inside the function itself, we want to write our code such that it can accept any list of possibilities as an argument and work from that.\n# set up a psudo argument pet_vector = survey$pets # make new dataframe for output pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector), \u0026#34;dog\u0026#34; = NA, \u0026#34;cat\u0026#34; = NA, \u0026#34;fish\u0026#34; = NA, \u0026#34;bird\u0026#34; = NA, \u0026#34;reptile\u0026#34; = NA, \u0026#34;rock\u0026#34; = NA, \u0026#34;none\u0026#34; = NA, \u0026#34;other\u0026#34; = NA) # get a binary for each known pet type pet_output$dog = grepl(pattern = \u0026#34;dog\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$cat = grepl(pattern = \u0026#34;cat\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$fish = grepl(pattern = \u0026#34;fish\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$bird = grepl(pattern = \u0026#34;bird\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$reptile = grepl(pattern = \u0026#34;reptile\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$rock = grepl(pattern = \u0026#34;rock\u0026#34;, x = pet_vector, ignore.case = TRUE) pet_output$none = grepl(pattern = \u0026#34;none\u0026#34;, x = pet_vector, ignore.case = TRUE) # remove all known pets and clean remaining text pet_vector = gsub(pattern = \u0026#34;dog\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;cat\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;fish\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;bird\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;reptile\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;rock\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;none\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = gsub(pattern = \u0026#34;,\u0026#34;, pet_vector, replacement = \u0026#34;\u0026#34;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#34;other\u0026#34; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#34;\u0026#34;, \u0026#34;other\u0026#34;] = NA Arbitrary Output The first step of our current code is to create a dataframe for our outputs. We still want to do that, but without us defining each possibility ourselves inside the function. Instead, we will provide a vector of possibilities, and have R iterate through those to make our columns. We can use a for() loop for that.\nFirst, weâll create a vector of our possibilities, in this case our pets:\npossible_columns = c(\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;bird\u0026#34;, \u0026#34;reptile\u0026#34;, \u0026#34;rock\u0026#34;, \u0026#34;none\u0026#34;) Next, we need code to iterate through those possibilities, and create a dataframe from them. Weâll start with making what we know, a column for IDs which has as many rows as our intended input, pet_vector from above. Next, we will iterate through all possible options, and make a new column for each. Here I iterate through our possible_columns vector, and for each element (option in the loop) I create a column of NAs.\n# make a base dataframe with rows for each of our cases. pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. pet_output[, as.character(option)] = NA } If we look at out output now, it is exactly the same as if we made each column ourselves, but now it is done by providing a vector of options. And we can change those options to whatever we want. This will come in handy later.\npet_output id dog cat fish bird reptile rock none 1 1 NA NA NA NA NA NA NA 2 2 NA NA NA NA NA NA NA 3 3 NA NA NA NA NA NA NA 4 4 NA NA NA NA NA NA NA 5 5 NA NA NA NA NA NA NA 6 6 NA NA NA NA NA NA NA 7 7 NA NA NA NA NA NA NA 8 8 NA NA NA NA NA NA NA 9 9 NA NA NA NA NA NA NA 10 10 NA NA NA NA NA NA NA 11 11 NA NA NA NA NA NA NA 12 12 NA NA NA NA NA NA NA 13 13 NA NA NA NA NA NA NA 14 14 NA NA NA NA NA NA NA 15 15 NA NA NA NA NA NA NA 16 16 NA NA NA NA NA NA NA 17 17 NA NA NA NA NA NA NA 18 18 NA NA NA NA NA NA NA 19 19 NA NA NA NA NA NA NA 20 20 NA NA NA NA NA NA NA 21 21 NA NA NA NA NA NA NA 22 22 NA NA NA NA NA NA NA 23 23 NA NA NA NA NA NA NA 24 24 NA NA NA NA NA NA NA Test for Each Option Our next step is to test for each possible option (in this case types of pets) and fill in the respective columns. We will use iteration here as well.\nUsing the same principle as above, iterate over each option in possible_columns and use grepl() to test if the pet appeared in that case. Fill the respective columns.\nThe following code will iterate through possible_columns and replace the pattern grepl() is looking for with each option. It will test for that option, and save the results in the corresponding column.\nfor(option in possible_columns){ # fill dataframe iterativly. pet_output[ , option] = grepl(option, pet_vector, ignore.case = TRUE) } Remove the Known Options to Find âOtherâ Once we have our âknownsâ taken care of, we can work on the others. The process is nearly identical, just swap grepl() with gsub() and apply it to pet_vector like before.\nIterate over each option in possible_columns and use gsub() to remove all of our known possibilities (and commas) from pet_vector. You can then use trimws() to remove the extra spaces. Assign the remaining values to the âotherâ column of pet_output.\nThe following will remove all known possibilities, clean the remainder, and assign it to the âotherâ column.\nfor(option in possible_columns){ # remove all known options pet_vector = gsub(pattern = option, pet_vector, replacement = '', ignore.case = TRUE) } # clear commas and whitespace pet_vector = gsub(pattern = ',', pet_vector, replacement = '', ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in 'other' pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == '', 'other'] = NA Turn it Back into a Function If we look at our code all together now, it looks like the following. If we run it, it will return the exact same thing as our old pet_split() function, but instead of each option being hand-coded by us, it knows how to work with any given vector of options and create our desired output.\n# make dummy argument pet_vector = survey$pets # set all known options possible_columns = c(\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;bird\u0026#34;, \u0026#34;reptile\u0026#34;, \u0026#34;rock\u0026#34;, \u0026#34;none\u0026#34;) # make a base dataframe with rows for each of our cases. pet_output = data.frame( \u0026#34;id\u0026#34; = 1:length(pet_vector) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. pet_output[, as.character(option)] = NA } # fill output df for(option in possible_columns){ # fill dataframe iterativly. pet_output[ , option] = grepl(option, pet_vector, ignore.case = TRUE) } # clear all know options for(option in possible_columns){ # remove all known options pet_vector = gsub(pattern = option, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) } # clear commas and whitespace pet_vector = gsub(pattern = \u0026#39;,\u0026#39;, pet_vector, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) pet_vector = trimws(pet_vector) # Fill in \u0026#39;other\u0026#39; pet_output$other = pet_vector # Turn blanks into NAs pet_output[pet_output$other == \u0026#39;\u0026#39; \u0026amp; !is.na(pet_output$other), \u0026#39;other\u0026#39;] = NA Convert our code back into a function, call the function comma_split().\ncomma_split = function(vector_to_split, possible_columns){ # make a base dataframe with rows for each of our cases. output = data.frame( \u0026quot;id\u0026quot; = 1:length(vector_to_split) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. output[, as.character(option)] = NA } # fill output df for(option in possible_columns){ # fill dataframe iterativly. output[ , option] = grepl(option, vector_to_split, ignore.case = TRUE) } # clear all know options for(option in possible_columns){ # remove all known options vector_to_split = gsub(pattern = option, vector_to_split, replacement = \u0026quot;\u0026quot;, ignore.case = TRUE) } # clear commas and whitespace vector_to_split = gsub(pattern = \u0026quot;,\u0026quot;, vector_to_split, replacement = \u0026quot;\u0026quot;, ignore.case = TRUE) vector_to_split = trimws(vector_to_split) # Fill in \u0026quot;other\u0026quot; output$other = vector_to_split # Turn blanks into NAs output[output$other == \u0026quot;\u0026quot; \u0026amp; !is.na(output$other), \u0026quot;other\u0026quot;] = NA # return output return(output) } comma_split(vector_to_split = survey$pets, possible_columns = c(\u0026quot;dog\u0026quot;, \u0026quot;cat\u0026quot;, \u0026quot;fish\u0026quot;, \u0026quot;bird\u0026quot;, \u0026quot;reptile\u0026quot;, \u0026quot;rock\u0026quot;, \u0026quot;none\u0026quot;)) Once you have the function created, try it on another column! Your output should match mine.\ncomma_split(vector_to_split = survey$tea_days, possible_columns = c(\u0026#34;monday\u0026#34;, \u0026#34;tuesday\u0026#34;, \u0026#34;wednesday\u0026#34;, \u0026#34;thursday\u0026#34;, \u0026#34;friday\u0026#34;, \u0026#34;saturday\u0026#34;, \u0026#34;sunday\u0026#34;)) id monday tuesday wednesday thursday friday saturday sunday other 1 1 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 2 2 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 3 3 TRUE TRUE TRUE FALSE FALSE TRUE TRUE \u0026lt;NA\u0026gt; 4 4 TRUE TRUE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 5 5 FALSE TRUE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 6 6 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 7 7 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 8 8 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 9 9 TRUE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 10 10 FALSE FALSE FALSE TRUE TRUE FALSE FALSE \u0026lt;NA\u0026gt; 11 11 FALSE FALSE FALSE TRUE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 12 12 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 13 13 TRUE TRUE TRUE TRUE TRUE FALSE TRUE \u0026lt;NA\u0026gt; 14 14 TRUE FALSE FALSE TRUE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 15 15 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 16 16 FALSE FALSE FALSE FALSE TRUE TRUE FALSE \u0026lt;NA\u0026gt; 17 17 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 18 18 FALSE FALSE FALSE FALSE FALSE FALSE TRUE \u0026lt;NA\u0026gt; 19 19 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 20 20 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 21 21 TRUE TRUE TRUE FALSE TRUE TRUE TRUE \u0026lt;NA\u0026gt; 22 22 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 23 23 FALSE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; 24 24 TRUE FALSE FALSE FALSE FALSE FALSE FALSE \u0026lt;NA\u0026gt; While it now has a bit of an odd name, our function can now work on any column! It is hard to express how big of a deal that is. We now have a single general tool that can adapt itself to several situations. The input is arbitrary, as long as it is formatted the same way (values separated by commas), we can put anything through this function and get a nice tidy dataframe back. A whole new universe of possibilities just opened.\nTry our function on some other columns in the survey dataframe!\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/09_apply_lists/","title":"List and Apply","tags":[],"description":"","content":" Overview The Data Apply comma_split() Read ALL the Files! Conclusion Overview The apply family of functions can be a handy way to perform repetitive tasks quickly. Today we will be using apply functions to improve some previous tasks. First we will be revisiting comma_split() (for the last time) and learning to use it on several columns at once. Next, we will be tackling a common annoyance when it comes to loading several files into R.\nThe Data We are going to be using class survey data for lab today. Please load it in using the following:\nsurvey = readRDS(url(\u0026#34;https://github.com/Adv-R-Programming/Adv-R-Reader/raw/main/class_survey.rds\u0026#34;)) Apply comma_split() The first thing we will be doing today is combining our comma_split() function and lapply() in order to split all of our \u0026lt;DRINK\u0026gt;_day comma separated columns at once. Iâve provided the fully generalized version of comma_split() weâve developed in the past few worksheets below. Run the following to add it to your environment.\ncomma_split = function(vector_to_split, possible_columns){ # make a base dataframe with rows for each of our cases. output = data.frame( \u0026#39;id\u0026#39; = 1:length(vector_to_split) ) # iterate through all options and create a column with NAs for it for(option in possible_columns){ # make a new column with a character version of each possible option. output[, as.character(option)] = NA } # fill output df for(option in possible_columns){ # fill dataframe iterativly. output[ , option] = grepl(option, vector_to_split, ignore.case = TRUE) } # clear all know options for(option in possible_columns){ # remove all known options vector_to_split = gsub(pattern = option, vector_to_split, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) } # clear commas and whitespace vector_to_split = gsub(pattern = \u0026#39;,\u0026#39;, vector_to_split, replacement = \u0026#39;\u0026#39;, ignore.case = TRUE) vector_to_split = trimws(vector_to_split) # Fill in \u0026#39;other\u0026#39; output$other = vector_to_split # Turn blanks into NAs output[output$other == \u0026#34;\u0026#34; \u0026amp; !is.na(output$other), \u0026#39;other\u0026#39;] = NA # return output return(output) } Once youâve got the function in your environment, we are going to use lapply() to apply it over all the relevant columns in our survey dataframe. The first argument to lapply() will be the thing you want to apply over. In this case it will be the survey dataframe. The second argument, FUN, is the function you want to apply to the first argument, comma_split().\nNote that when supplying a dataframe to lapply() it interprets this as applying FUN to each column in the dataframe.\nThis covers what we want to apply (FUN) and what we want to apply it to (survey), but there is one hangup. How do we pass our arguments to comma_split()? The first argument of FUN is always assumed to be the current part of your data the apply function is going over. In the case of survey and comma_split() the first argument to comma_split(), vector_to_split, will be the columns from survey.\nBut how do we specify the rest of the arguments? This is where some of our advanced function knowledge comes in. The third argument to lapply() is ..., which you may recall means we can pass arguments through the lapply() function. As long as we put in an argument to lapply() that matches the argument names in FUN, (comma_split()), they will be passed through. So in this case if we supply lapply() with an argument called possible_columns, that will be passed to comma_split().\nUse lapply() to apply comma_split() to all of the \u0026lt;DRINK\u0026gt;_days columns in our survey dataframe. Save the results as drink_dfs. DO NOT include the () after comma_split when providing it as an argument; it will produce an error.\ndrink_dfs = lapply(X = survey[, c(âcoffee_daysâ, âtea_daysâ, âsoda.pop_daysâ, âjuice_daysâ, ânone_daysâ)], FUN = comma_split, possible_columns = c(âmondayâ, âtuesdayâ, âwednesdayâ, âthursdayâ, âfridayâ, âsaturdayâ, âsundayâ))\nYou will have gotten a list object of length 5 back. Recall that lists are super-vectors. In this case, each element of our list contains an entire dataframe! Each of these dataframes is the normal output from comma_split(). We could combine these with our survey data if we cared to dome some analyses.\nRead ALL the Files! Using lapply() does not need to be limited to uses within R. One common way to use it is when loading in data. To try this, weâre going to load in a number of .csv files containing economic and population data on Massachusetts from the American Community Survey five-year estimates (ACS5). You can think of it as a yearly mini-census.\nFirst, we need to have a directory of data files with identical structures. For this example, we can use some data I have hosted on GitHub. Run the following code to download the zip archive into your project directory and un-zip it.\n# download file into project directory download.file(\u0026#34;https://github.com/Adv-R-Programming/Adv-R-Reader/raw/main/content/class_worksheets/09_apply_lists/09_data.zip\u0026#34;, \u0026#34;./09_data.zip\u0026#34;) # unzip into folder unzip(\u0026#34;./09_data.zip\u0026#34;, exdir = \u0026#34;./09_data_dir/\u0026#34;) Now that we have the files, we can treat them like any other .csvs we may have used for data analysis. Typically, you would need to run read.csv() to load in these files one at a time. That is a chore. Instead, weâll combine a few of the skills weâve learned in the worksheets thus far.\nFirst, get a vector of all the file paths of the data inside that folder using list.files(). I use the pattern argument here to say âI only want files that contain this.â In this case, it is the âecon_â prefix so I only get our âecon_acs5_YEAR.csvâ files which contain economic data.\necon_data_paths = list.files(\u0026#34;./09_data_dir/\u0026#34;, pattern = \u0026#34;econ_\u0026#34;, full.names = TRUE) Now, we will use lapply() to read in all of the econ_X dataframes at once.\nall_econ_data = lapply(econ_data_paths, read.csv) You now have a list of length 6 with all of the econ data! No copy and pasting required. Take a moment to look through the list and see how it is structured.\nYou may notice each element is a dataframe in the long format (key-value pairs). Rather than try to re-format them one at a time, we can pivot and merge them all at once. First, Iâll pivot everything from long to wide:\nlibrary(tidyr) all_econ_data_wide = lapply(all_econ_data, FUN = pivot_wider, id_cols = c(\u0026#34;GEOID\u0026#34;, \u0026#34;NAME\u0026#34;), names_from = \u0026#34;variable\u0026#34;, values_from = c(\u0026#34;estimate\u0026#34;, \u0026#34;moe\u0026#34;)) Next, weâll use basename() to give each dataframe a year identifier (weâll just use the file name for now). Iâll do this one in a for() loop as itâs easier to match the file names vector and our data list elements. Here we are combining our file function and list knowledge.\nfor(i in 1:length(all_econ_data_wide)){ # get the file name I want file_name = basename(econ_data_paths[i]) # add that as a column to the matching list element all_econ_data_wide[[i]]$file_name = file_name } Now we can bind them all together. Binding a dataframe is one method to join them together. Here Iâll use a function called do.call(). This is really this is the only time I ever use it, and I donât know what else it is helpful for, but in this case it allows us to take all component dataframes of our list, and bind them all together at once.\nmerged_econ = do.call(rbind, all_econ_data_wide) Tada! Rather than needing to load everything in one at a time, clean it one at a time, and combine them one at a time, using the power of apply we have completed each process by writing the code once, and applying it to several objects.\nRepeat this process with the âpop_acs5_XXXXâ CSVs.\nlab_3_data = âpath/to/lab-3-tidy-agg-merge-NAME/data/â\npop_data_paths = list.files(lab_3_data, pattern = âpop_â, full.names = TRUE)\nall_pop_data = lapply(pop_data_paths, read.csv)\nall_pop_data_wide = lapply(all_pop_data, FUN = pivot_wider, id_cols = c(âGEOIDâ, âNAMEâ), names_from = âvariableâ, values_from = c(âestimateâ, âmoeâ))\nfor(i in 1:6){\n# get the file name I want file_name = basename(pop_data_paths[i])\n# add that as a column to the matching list element all_pop_data_wide[[i]]$file_name = file_name }\nmerged_pop = do.call(rbind, all_pop_data_wide)\nConclusion Becoming comfortable with iteration, and apply especially, takes time. However, when combined with your ability to write your own functions this is where you start to see the power of working in code. You can solve a task once and apply that solution to dozens or hundreds of cases at once.\nYou can use the same process we went through here to look at your music collection and rip meta-data. You could analyze your photo collection. Generate and save text. Whatever you want. Let the mechanisms we have learned here start framing what you want to do as a final project. You now know how to build the scaffold to nearly any project.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/11_web_scrape/","title":"Web Scraping","tags":[],"description":"","content":" Overview A Legal Refresher (not legal advice) The Target Checking the Rules Figuring out the Structure Building our Bot Names Positions Links The Next Level Overview Web scraping is a handy skill to have if you want to create a dataset of your own. If you want to create a package that creates new datasets, this will be very helpful. If not, it is still a great way to practice making your own functions.\nToday weâll practice scraping some information about the SDS faculty from the Smith website. The same principles can be applied to any simple website (those without interactive elements). Just remember that you must be very careful when constructing scraper bots to make sure you obey the terms of service for the site you are scraping, and that the bots you build are polite.\nFailure to build polite bots can result in your (or the schoolâs) IP address being banned from a website forever.\nA Legal Refresher (not legal advice) Remember from our lecture that the legalities of web scraping are in a grey area. The legalities depend on several factors including:\nThe kind of data you are trying to get How you are getting and saving it What you plan to do with the data once you have it In general, you should never scrape:\nAnything under copywrite Anything about private people Anything you need to log-in to see The Target Rather than working with a dataset today, you will be making one. Our goal is to start with the Statistical \u0026amp; Data Sciences webpage, and end up with a dataframe containing the name, title, and URL for all the Smith SDS faculty.\nChecking the Rules Before we start writing any code, we need to make sure we are allowed to scrape the website. A good first check is the robots.txt of the website. For Smithâs site, that would be https://www.smith.edu/robots.txt. On this page we are given a map of where we are allowed and not allowed to scrape. Anything page listed with âDisallowâ is off limits, as is anything under that directory. For example, if www.site.com/page1 is off limits, so is www.page.com/page1/sub_page. Seems the parent directory of the SDS page, www.smith.edu/academics, is in the clear.\nOnce that quick check is passed, we have to do the harder work of reading and understanding the website Terms of Service (ToS); you can find Smithâs here. Yes, you actually need to read it. You are actually bound by these terms just by looking at the site, so it is worth the time.\nThe first thing you may notice is under section 4, that we are authorized to save pages from the site onto one single hard drive. That conversely means we are not able to save multiple copies, or save it anywhere that will be shared online. You may also notice that section 7.2 mentions scraping, in that we cannot use it to access parts of the site not made publicly available. What we want is publicly available, so we should be in the clear.\nWhat other things do you find interesting regarding the Terms of Service for the Smith website?\nFiguring out the Structure Now that we know what we are and are not allowed to do, letâs go look at the SDS page. Our goal here is to figure out what we need to get from this page to progress us closer to our goal. We know we want to look at each individual faculty box, so what could we do to get a link to all of them?\nWe could just go to each faculty member, copy the data into an excel file, and proceed from there. That would work for the 15 items we want here, but what if we wanted 150? What if we wanted to re-use our code for another department? Best to write a programmatic solution.\nMost of the information we want is clearly visible, but the link to each faculty page is not. We know that clicking on each faculty portrait will take us to their page, so there is a link in there somewhere, we just need to get it out. Weâll use SelectorGadget to help with that. First, weâll need to add it to our bookmarks bar. Right click on the bookmarks bar in your browser of choice, and make a new bookmark called âSelectorGadgetâ, and set the URL to the following:\njavascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})(); That looks like gibberish to us, but the computer will get it.\nOnce that is done, save the bookmark, and get back to the SDS page. While you are on the page, click on the âSelectorGadgetâ bookmark in your bar. A grey and white bar will appear in the lower right of your browser, and orange boxes will start to appear wherever you have your mouse. Click on the thing you want to find, in this case the name of a faculty member; it doesnât matter which one, just make sure the highlight box is only around the name. If you did it right, the name you clicked on will highlight green, and the rest will highlight yellow like the following.\nWeâre not done yet though. Look around and make sure we havenât included anything extra.\nLook over the page and make sure only the names are highlighted. If anything else is highlighted, click on it to turn it red and exclude it.\nThe Program Committee header needs to be excluded.\nLook at the SelectorGadget grey box in the lower right and you will see a short string starting with .fac-inset, thatâs the HTML section for our names. Copy the whole string and keep it somewhere handy.\nBuilding our Bot Names The first step of our scraping is to find the names for all the faculty. The dataframe we create will also be used to store the rest of our data later.\nStart by loading rvest, and scraping the whole SDS page into R. This can be done with the read_html() function, much like reading a CSV. Save the webpage to an object called sds_home. It is important to note this is the step that can get us in trouble. Once we have the page in R, we are working with it like anything else on our computer. But the process of reading the page from the internet can cause problems if we do so too fast. From the robots.txt page, we know Smith only wants us to pull one page every 10 seconds. If we do any more, they can ban us from the website. Be careful!\nScrape the SDS home page into R and store it in an object called sds_home.\nlibrary(rvest)\nsds_page = read_html(âhttps://www.smith.edu/academics/statisticsâ)\nOnce we have the whole page, we can start pulling information from it. The usual workflow here is to tell R what HTML structure we are interested in, and then what we want from it. For example, we can say we want the names of faculty using the selector path we found previously.\nTo do that, we need to say âfrom this page, look at this structure, and give me the contents.â In R, that corresponds to the html_elements() and html_text2() functions. Give our sds_home object to html_elements(), and as an argument, specify that css = the string we got from SelectorGadget earlier. Either pipe the results from that, or wrap it in the html_text2() function to get the actual names of the faculty.\nThere is also a html_element() function (singular, not plural). This will only give you the first thing.\nCreate a dataframe called sds_faculty with a column called name for all the SDS faculty names.\nsds_faculty = data.frame(ânameâ = html_text2(html_elements(sds_page, â.fac-inset h3â)))\nPositions Next we want to get the titles for all the faculty. The process is exactly the same as the above, but we need to set a different target using SelectorGadget\nUsing the same process as before, add the title of each faculty member to our sds_faculty dataframe into a new column called title.\nsds_faculty$title = html_text2(html_elements(sds_page, â.fac-inset pâ))\nLinks This last one will be a bit different. Rather than wanting the actual text on the page, we want the link that the text is tied to; i.e.Â when we click on a faculty name, it follows a link to their individual pages. Rather than using html_text2(), we will use the more general html_attr() function. This lets us have more control to tell R that we want the link the text is representing, not the text itself.\nIn HTML speak, the page a link points to is designated by the href, or Hypertext Reference. We need to tell R that is what we want. We can do that by passing âhrefâ to the name argument of html_attr(); âhrefâ is the name of the attribute we want to get.\nUsing html_elements() and html_attr(), get the links from the faculty names and add them to our dataframe in a column called link.\nsds_faculty$link = html_attr(html_elements(sds_page, â.linkopacityâ), name = âhrefâ)\nThe Next Level You may be thinking âthatâs kind of neat, but it doesnât tell me anything I canât see with my own eyes.â Youâd be right. However, in a typical web scraping process, this is only step one. We now have a column of all the links for each of the individual faculty pages. If we were to write code that iterates over those links, we could then get more specific info from each faculty member. We could add things like email, office location, educational history, etc. to our dataframe. Once we figure that out, we could also iterate over all of the departments at smith, and before you know it, we have a full blown database on our hands.\nWith this sort of power, you must be very careful. Be sure to build polite bots that obey the website rules, especially with how fast they iterate through pages. Always use the Sys.sleep() function to give your bot a break between each page. The Smith site specifically asks that you wait 10 second between each page, so your code should include a Sys.sleep(10) inside each iteration.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/12_regex/","title":"RegEx","tags":[],"description":"","content":" Overview Overview Regular expressions (RegEx) are a super handy tool to have in your tool kit. Whenever you have a problem with text, you can probably make a RegEx pattern to fix it. However, making those patterns is usually an exercise of trial and error. Thankfully, there are some great resources to make that process easier.\nThe first tool is Regex101, a site where you can interactively test out your regex patterns on a sample of text. We wonât be using it for anything today, but it is great to know about. Just note that any pattern you build here will need to be modified slightly for using in R. Specifically, any time you use a \\, you will need to double it to \\\\.\nWhat we will be using today is a great site called Regexlearn. For todayâs worksheet, your task is to work though levels 1-49 in the interactive tutorial. The site will dynamically update what your regex would do if used it as you type, so it offers immediate feedback.\nThatâs all there is for today!\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/14_parallel/","title":"Parallel","tags":[],"description":"","content":" Overview Baseline Sequential Execution Setting a Parallel Plan Simple Parallel Whatâs the Catch? Data Transfer I/O Constraints Dependency A Note on Shared Environments Conclusion Overview Parallelizing code can lead to some impressive performance gains, but it is not a silver bullet. Not only does your problem need to be easily compartmentalized to run in parallel, but bottlenecks in your code flow can actually make code slower if run in parallel. Today weâll run a few tests to try and get a sense of when it is and is not helpful to parallelize our code.\nYou are going to need the future.apply and tictoc packages for the worksheet today, so be sure to install them.\nBaseline Sequential Execution Before we get into running code in parallel, letâs establish a baseline for your machine. Execute the following code to define a function that will find prime numbers between 1 and n.\n# Function taken from John on this stack overflow post # https://stackoverflow.com/questions/3789968/generate-a-list-of-primes-up-to-a-certain-number getPrimeNumbers \u0026lt;- function(n, dead_weight) { n \u0026lt;- as.integer(n) if(n \u0026gt; 1e6) stop(\u0026#34;n too large\u0026#34;) primes \u0026lt;- rep(TRUE, n) primes[1] \u0026lt;- FALSE last.prime \u0026lt;- 2L for(i in last.prime:floor(sqrt(n))) { primes[seq.int(2L*last.prime, n, last.prime)] \u0026lt;- FALSE last.prime \u0026lt;- last.prime + min(which(primes[(last.prime+1):n])) } which(primes) } To get our baseline, weâre going to iterate over a numeric vector, and calculate all the prime numbers for those values. We donât actually need to save these values for anything, so Iâll just assign them to primes and the overwrite it.\nRun the following code and write down how long it takes to execute somewhere easy to reference later. You need to execute all of it at once for the tic-toc timer to work!\n# load in tictoc for easy benchmarking library(tictoc) # start timer tic() # set our vector to iterate through num_vec = 10:9001 # get primes for(i in num_vec){ primes = getPrimeNumbers(i) } # stop timer toc() For a comparison, letâs run the same code as a regular apply function.\nRun the following code and write down how long it takes to execute somewhere easy to reference later. You need to execute all of it at once for the tic-toc timer to work! There should be a small improvement over the for loop.\n# start timer tic() # set our vector to iterate through num_vec = 10:9001 # get primes primes = sapply(num_vec, getPrimeNumbers) # stop timer toc() The time difference in this case will be minimal given the small scale of our code. For example, the for loop for me was 22.78 seconds, while the sapply() was 21.19 seconds; a difference of ~7%1. 7% of a few seconds in negligible, but 7% of an hour (~4 minutes) is time to get a snack. 7% of a work day (~33 mins) gets you an extra lunch break. 7% of a week may get you your results a full half work day early! Letâs take it even further.\nSetting a Parallel Plan To prepare for running our first bit of code in parallel, we need to understand the specs of your machine a bit better. It is completely possible to crash your computer if you ask it to run too much code at once. One of the most common culprits of this is asking your computer to use too many cores.\nTo determine how many cores you have on your computer, run the following and keep track of the number.\n# Get the number of cores on this machine parallel::detectCores() # the max cores we should possibly use max_safe = parallel::detectCores() - 1 You could theoretically run as many streams of code as you have cores, but you really shouldnât. To find the max, take whatever number you just got and lower it by one. This will always leave one core free for your computer to do other important things, like let you move the mouse.\nOnce we have our maximum safe number in mind, we can start preparing for running code in parallel. Weâll be using the future package to coordinate our parallel code; more specifically the future.apply package. future lets you write parallel code in a generic way, and then adapt how the code is actually executed later. This can be really helpful if you swap between machines often like I do (or how we will next week).\nTo create our parallel plan, all we need to do is load the future.apply package, and run the plan() function. The options for our plan are as follows:\nName OSes Description synchronous: non-parallel: sequential all sequentially and in the current R process asynchronous: parallel: multisession all background R sessions (on current machine) multicore not Windows/not RStudio forked R processes (on current machine) cluster all external R sessions on current, local, and/or remote machines R normally runs sequentially, meaning it runs code one thing at at time in order. We can specify that as our plan if we wanted, but then nothing would really change. The most robust plan type is multisession, where we create multiple copies of R to handle different sections of our code at once; weâll do that here. To specify that is our plan, run the following code. Youâll notice I also set the argument workers to 2. This means we essentially will have 2 copies of R working at the same time for us.\n# load the package library(future.apply) # set our plan plan(multisession, workers = 2) Simple Parallel Now that we have our plan set, letâs run our code from above again. This time, weâll use the future_sapply() function to run it in parallel.\nRun the following code and write down how long it takes to execute somewhere easy to reference later.\n# start timer tic() # set our vector to iterate through num_vec = 10:9001 # get primes primes = future_sapply(num_vec, getPrimeNumbers) # stop timer toc() You should see a more sizable decrease in execution time, for me it went from 21.19 seconds with a regular apply, to 14.92 seconds with 2 workers. What happens if we add more workers?\nRun the following code and write down how long it takes to execute somewhere easy to reference later. Notice that Iâve changed our plan to 3 workers.\n# start timer tic() # change out plan to 3 workers plan(multisession, workers = 3) # set our vector to iterate through num_vec = 10:9001 # get primes primes = future_sapply(num_vec, getPrimeNumbers) # stop timer toc() Down to 12.19 second for me, including the time it takes to change out plan! Letâs go full-bore and use our maximum safe number of cores from above.\nRun the following code and write down how long it takes to execute somewhere easy to reference later. Notice that Iâve changed our plan to 3 workers.\n# start timer tic() # change out plan to max safe workers plan(multisession, workers = max_safe) # set our vector to iterate through num_vec = 10:9001 # get primes primes = future_sapply(num_vec, getPrimeNumbers) # stop timer toc() Working with as many cores as I can spare, the code got down to 8.14 seconds. Thatâs a full 80% faster, which is a huge deal!\nWhatâs the Catch? We can get such large improvements with this code because it is simple, and there are no bottlenecks in our code. In general, a bottleneck is the slowest part of a system, so called because everything else has to wait for it to finish. For example, say we are thinking of building a new desktop computer. We need to select our new CPU, hard drive, and RAM. All three of these are reliant on each other for the computer to work quickly. If we spend all of our funds on getting the fastest possible CPU, but buy a really slow hard drive, then the CPU will just be sitting around waiting for the hard drive to relay whatever data the CPU requested.\nThe main purpose of running code in parallel is to get around the bottleneck of only being able to execute one piece of code at a time. By running code in parallel on multiple cores, we can do several computations at once, meaning we can get to the next step faster. However, there is almost always a next step, and it may not be parallelizable.\nData Transfer One of the most common bottlenecks is caused by data transfer. We we start all these workers for R, each one needs to get a full copy of the data before it can start working. Try running the following, where I include a large data object in our sapply() as an argument.\nRun the following code and write down how long it takes to execute somewhere easy to reference later.\n# start timer tic() # change out plan to max safe workers plan(multisession, workers = max_safe) # set our vector to iterate through num_vec = 10:9001 # make dead weight dead_weight = matrix(1:9999999) # get primes primes = future_sapply(num_vec, getPrimeNumbers, dead_weight) # stop timer toc() You should see that the time to run has stared creeping back upwards. This was a relatively simple example, but as the data to transfer gets larger, or the steps that canât be parallelized become more common, we start to lose the gains parallel code gets us.\nI/O Constraints The above example has to do with the limits of our CPU. We can also run in to trouble with other parts of our hardware. For example, we can make more workers to do computations for us, but if one of our steps is loading data, it may not matter. This is because we run into the limits of our hard drives. No matter how many workers we have, if we hit the speed limit on our hard drive loading data in to R, all those workers will just have to sit and wait.\nDependency It is also important to note that none of this works if there is dependency in our code. All tasks to be done in parallel need to be independent from each other, otherwise we canât run several of them at once.\nA Note on Shared Environments Thus far I have assumed you are working on your own personal computer. If you task is large enough that you need to parallelize things, you may be working on a server or some other shared environment. It is critical to note in these situations that running your code in parallel can really make things difficult for other people.\nConsider our above code to determine the maximum safe number of workers to use. We just took the number of cores our machine has and subtracted one so we could keep using it while the code runs. If you do this on a server, you will be taking every single core that every other person is using, and then just leaving one so you can still check the status. In effect, you take this massive shared resource and say: âthis is all mine.â That is what is called a jerk move.\nThe server admins will notice and they may not be happy.\nConclusion Running code in parallel is cool, and can be helpful. But it is important to know the limitationsâand when it can actually be detrimental. Be sure you keep these conditions in mind when you decide to parallelize code or not.\nThe actual percent things will differ relies on a ton of factors. Donât take this as a representative number.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/class_worksheets/15_pdf_data/","title":"PDF Data Extraction","tags":[],"description":"","content":" Overview Install Guide pdftools tesseract tabulizer Optical Character Recognition + Text Extraction Area Selection \u0026amp; Tables Conclusion Overview PDFs are a ubiquitous file format. However, they were not made with data processes in mind. This can make them quite a pain to work with, so they should be avoided when possible.\nToday I will walk you through one example of when I needed to get data out of PDFs. It will be less of a worksheet and more of a demonstration. In 2019 I spent some time working in the UK building a machine learning system to identify what foster care providers should be prioritized for inspection to assure children were receiving adequate care. One of the most important bits of data in building this system was how these fostering agencies have performed in the past.\nThe governmental agency that oversees this process, Ofsted, regularly published reports on these providers to the public online. However, they were only available in PDF format. To make this data useful, step one was building a web scraper to download all of the reports and attach relevant metadata. The next step was scraping the contents from the PDFs. The last step was using that raw data for further analyses. Today weâre going to work on that second PDF extraction step together.\nIâve provided a real report PDF at the following link. Please download the file and place it in the data directory of your current project.\nDownload the PDF Report Here.\nInstall Guide Working with PDFs requires a few specialized tools. Iâll try to walk you though installing them here. In total, we need to end up with working versions of the tesseract, pdftools, and tabulizer packages. Iâll provide links to their home pages here:\npdftools pdftools homepage\npdftools should be the easiest to install, as it does not have any special dependencies unless you are on Linux. You should be able to install it using the following:\ninstall.packages(\u0026#34;pdftools\u0026#34;) tesseract tesseract homepage\nThe tesseract package works as a wrapper around tesseract-ocr. It should also install without difficulties for most systems using the following code:\ninstall.packages(\u0026#34;tesseract\u0026#34;) tabulizer tabulizer homepage\ntabulizer is where the difficulties will come in, as it relies on Java and is currently out of date on CRAN. This means we will need to install it directly from GitHub. Run the following code to install the required packages. If rJava fails to install make sure you have a valid java installation on your system. Make sure you install the appropriate version for your operating system.\n# install remotes install.packages(\u0026#34;remotes\u0026#34;) # install rJava install.packages(\u0026#34;rJava\u0026#34;) # on 64-bit Windows remotes::install_github(c(\u0026#34;ropensci/tabulizerjars\u0026#34;, \u0026#34;ropensci/tabulizer\u0026#34;), INSTALL_opts = \u0026#34;--no-multiarch\u0026#34;) # elsewhere remotes::install_github(c(\u0026#34;ropensci/tabulizerjars\u0026#34;, \u0026#34;ropensci/tabulizer\u0026#34;)) Optical Character Recognition + Text Extraction The first problem with our PDF is that it is a flattened document, which means the content is really just a picture. You can test this yourself by opening the PDF in your program of choice and trying to select the text. We need to re-digitize this text.\nOur first step then is running Optical Character Recognition (OCR) on the document. We can do this in R using the tesseract package. tesseract will look at the document, and try to recognize the squiggles as letters and words. This is much harder for a machine than it is for us, but this document is pretty clean so it should do well.\nIf we have tesseract installed, then pdftools will automatically use it if we ask for text from a flattened document. We can do this using the pdf_ocr_text() function and by providing the path to the PDF. The following code will turn every page into a .png image, then try to recognize all the text on the page. It will return a character vector where each page is an element in the vector.\nRun the following code to run OCR on the example PDF and extract the text as a blob per page.\n# get text blobs pdf_text_vector = pdftools::pdf_ocr_text(\u0026#34;data/SC_SC037304_2007_07_30__1.pdf\u0026#34;) Looking at out output we get a character vector of length 13 (one for each page), inside which the contents of each page are all smushed together. From this, I can try to extract the information I want using some RegEx. Iâll provide the specific formulas you need; do try to understand them though! Iâm using regex101 to help me build my formulas.\nI want to make a dataframe with one row (for this report), with columns indicating the name of the foster care agency, itâs reference number, the date, and the summary result. If we were really building a database from this data, we would then run this code on other reports such that each report would create one row in the database.\nFirst, I need to load in stringr for text tools, and combine all of my pages into one giant text blob. This is so that I can search for the info I want from the entire document, rather than needing to search on each page.\n# load stringr library(stringr) # combine all the elements of the vector into one large element for searching pdf_text_blob = paste0(pdf_text_vector, collapse = \u0026#34; \u0026#34;) Next, I start looking for my key data points. I start with the reference number, then get the date, and summary rating. I then take those elements and make a dataframe.\n# Get the document URN # the regex formula is looking for whatever comes after the phrase \u0026#34;Unique reference number\u0026#34; and ends with the next newline (\\n) character # I need to subset the results matrix to just the contents of my capture (inside the () ), rather than everything doc_urn = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=Unique reference number) (.+?)\\\\n\u0026#34;)[1,2] # Get the date doc_date = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=Inspection date) (.+?)\\\\n\u0026#34;)[1,2] # get summary result doc_result = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=The overall quality rating is) (.+?)\\\\.\\\\n\u0026#34;)[1,2] # combine results doc_df = data.frame(\u0026#34;urn\u0026#34; = doc_urn, \u0026#34;date\u0026#34; = doc_date, \u0026#34;rating\u0026#34; = doc_result) Because I want to (theoretically) run this code on all the reports, I will turn it into a function.\nget_report_info = function(pdf_text_blob){ # Get the document URN # the regex formula is looking for whatever comes after the phrase \u0026#34;Unique reference number\u0026#34; and ends with the next newline (\\n) character # I need to subset the results matrix to just the contents of my capture (inside the () ), rather than everything doc_urn = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=Unique reference number) (.+?)\\\\n\u0026#34;)[1,2] # Get the date doc_date = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=Inspection date) (.+?)\\\\n\u0026#34;)[1,2] # get summary result doc_result = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=The overall quality rating is) (.+?)\\\\.\\\\n\u0026#34;)[1,2] # combine results doc_df = data.frame(\u0026#34;urn\u0026#34; = doc_urn, \u0026#34;date\u0026#34; = doc_date, \u0026#34;rating\u0026#34; = doc_result) # return return(doc_df) } get_report_info(pdf_text_blob = pdf_text_blob) You may notice that there is a mistake in the Unique reference number in my results; in that there is an extra $. Unfortunately that comes with OCR territory, and it did not quite read the document correctly. All we can do is keep an eye out for these issues and correct them. In this case, Iâll add a warning if I see any special characters in the URN.\nget_report_info = function(pdf_text_blob){ # Get the document URN # the regex formula is looking for whatever comes after the phrase \u0026#34;Unique reference number\u0026#34; and ends with the next newline (\\n) character # I need to subset the results matrix to just the contents of my capture (inside the () ), rather than everything doc_urn = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=Unique reference number) (.+?)\\\\n\u0026#34;)[1,2] # Get the date doc_date = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=Inspection date) (.+?)\\\\n\u0026#34;)[1,2] # get summary result doc_result = str_match(pdf_text_blob, \u0026#34;(?\u0026lt;=The overall quality rating is) (.+?)\\\\.\\\\n\u0026#34;)[1,2] # combine results doc_df = data.frame(\u0026#34;urn\u0026#34; = doc_urn, \u0026#34;date\u0026#34; = doc_date, \u0026#34;rating\u0026#34; = doc_result) # test for special characters in URN if(grepl(\u0026#34;[^a-zA-Z0-9_]\u0026#34;, doc_df$urn)){warning(paste0(\u0026#34;Non alpha-numeric symbol in URN! See: \u0026#34;, doc_df$urn))} # return return(doc_df) } get_report_info(pdf_text_blob = pdf_text_blob) Area Selection \u0026amp; Tables Say we didnât want to get the whole document back as a big blob. One of the most common times this is the case is when there is a table of data in our PDF. In this case, we have a table of text on page 10 of the report. It would be great to somehow keep that formatting and pull it into R as a table. Well, we can! This is where tabulizer comes in.\ntabulizer allows you to use the X/Y coordinates in a PDF to your advantage, and just get back the content from that area. Unfortunately, it does not work with flattened documents, so you would need to run OCR on the document outside of R. For today, download the following digital version of the document and add it to your data folder.\nDownload the digital PDF Report Here.\nRun the following code to start an interactive tool to extract the table from page 10. You should click and drag to create a box around the table. If the box is not in the right position, hover over the edge of the box to adjust it.\n# start interactive table too table_1 = tabulizer::extract_areas(\u0026#34;data/digital_SC_SC037304_2007_07_30__1.pdf\u0026#34;, pages = 10) How do the results look? If your results are like mine, it kinda got the idea, but leaves a lot to be desired. We can refine our extraction with a few steps. First, we need to get the X/Y coordinates of our table.\nRun the following code to start an interactive tool to get the location of our table. The interface will be the same as the above, but it will output a list which contains the dimensions of our table for use later.\n# start interactive table too table_1_coords = tabulizer::locate_areas(\u0026#34;data/digital_SC_SC037304_2007_07_30__1.pdf\u0026#34;, pages = 10) Now that we have our coordinates, we can refine our table extraction a bit. Using extract_tables() we can help the process by specifying how many columns there should be. I do this by setting guess to FALSE, and setting columns to the right boundary of each column. You can read the other options in the help page.\n# set my own coordinates for rendering table_coords = list(c(236, 70, 674, 525)) # manually refine table results doc_table = tabulizer::extract_tables(\u0026#34;data/digital_SC_SC037304_2007_07_30__1.pdf\u0026#34;, guess = FALSE, pages = 10, area = table_coords, columns = list(c(120, 424, 525)), output = \u0026#34;data.frame\u0026#34;)[[1]] The results should be a bit nicer, but still not perfect. We can combine those multiple rows into a single value with the following code. I take a step through my current output line by line, and if the next line has an NA in the standard column, I combine the contents of that rowâs Action column with the previous. I then create a clean output.\n# make output vector output = c() # walk through each row and combine non-NA row conent for(line in 1:nrow(doc_table)){ # check if standard is not NA if(!is.na(doc_table[line, \u0026#34;Standard\u0026#34;])){ # record standard standard = doc_table[line, \u0026#34;Standard\u0026#34;] # clear buffer for action text action_text = c() } # get the current action text action_text = append(action_text, doc_table[line, \u0026#34;Action\u0026#34;]) # if the next row is a new standard, combine current text and save in output if(line + 1 \u0026gt; nrow(doc_table) || !is.na(doc_table[line + 1, \u0026#34;Standard\u0026#34;])){ # combine all action text so far action_text = paste0(action_text, collapse = \u0026#34; \u0026#34;) # save to output output[as.character(line)] = action_text } } # make clean output clean_doc_table = data.frame(\u0026#34;standard\u0026#34; = doc_table[!is.na(doc_table$Standard), \u0026#34;Standard\u0026#34;], \u0026#34;action\u0026#34; = output, \u0026#34;due_date\u0026#34; = doc_table[!is.na(doc_table$Standard), \u0026#34;Due.date\u0026#34;]) Conclusion Working with PDFs is messy, but being able to do so gives you access to an entirely new realm of data sources. While it is always advisable to look for other ways to get data, if PDFs is all you have, at least now you know how to make the best of things.\n"},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/hidden/","title":"Hidden","tags":[],"description":"","content":""},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/credits/","title":"Credits","tags":[],"description":"","content":"Content Thank you to Dr. Ben Baumner for providing materials as reference as I prepared this course reader.\nBackend Thanks to the Hugo projects for providing the infrastructure for this site. Thank you to the team that created Hugo Learn Theme, the basis of this reader.\nPackages and libraries mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/","title":"Home","tags":[],"description":"","content":" Advanced Programming for Data Science Quick Links Resource Link Description Syllabus Quick access to all important course information. Moodle Grades and quizzes will be available on the course Moodle. Slack Main communication channel for the course. Spinelli Center The Spinelli Center offers drop-in tutoring hours in Sabin-Reed 301 or on Zoom. Office Hours Sign up for a slot in office hours here. Overview Info Value Who Dr. Jared Joseph What SDS 270: Advanced Programming for Data Science When Mon/Wed/Fri 9:25-10:40 Where Sabin-Reed 220 Schedule Below is the tentative schedule for the course. While we will try to keep to this schedule, unanticipated situations (and mountain day) may require us to adjust. Each row is a class meeting, with the readings and assignments due on that day listed.\nWeek Date Topic Readings Due 1 1/27/2023 (Fri) Roadmap Class Syllabus Update R/R Studio 2 1/30/2023 (Mon) Intro Git Bryan, J. (2018). Excuse Me, Do You Have a Moment to Talk About Version Control? American Statistician, 72(1), 20-27. Complete all Install Guides 2 2/1/2023 (Wed) Objects in R Wickham, H. (2020). 3 Vectors. In Advanced R. Chapman \u0026amp; Hall. 2 2/3/2023 (Fri) Lab 1 3 2/6/2023 (Mon) Functions Wickham, H. (2020). 6 Functions. In Advanced R. Chapman \u0026amp; Hall. 3 2/8/2023 (Wed) Debugging \u0026amp; Flow Bryan, J., \u0026amp; Hester, J. (2021). Chapter 11 Debugging R code. In What They Forgot to Teach You About R. Grolemund, G., \u0026amp; Wickham, H. (2017). 8 Conditions. In R for Data Science. O'Reilly. Lab 1 3 2/10/2023 (Fri) Lab 2 4 2/13/2023 (Mon) Iteration [21.1-21.3 Only] Wickham, H., \u0026amp; Grolemund, G. (2017). 21 Iteration. In R for Data Science. O'Reilly. 4 2/15/2023 (Wed) Apply \u0026amp; Lists Lab 2 4 2/17/2023 (Fri) Lab 3, Quiz 1 Open 5 2/20/2023 (Mon) Web Scraping Irizarry, R. A. (2022). Chapter 24 Web scraping. In Introduction to Data Science. Zimmer, M. (2010). But the data is already public: On the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313-325. 5 2/22/2023 (Wed) RegEx Lab 3 5 2/24/2023 (Fri) Lab 4 Quiz 1 Due 6 2/27/2023 (Mon) Parallel Jones, M. (2017). Quick Intro to Parallel Computing in R. Peng, R. D. (2022). 22 Parallel Computation. In R Programming for Data Science. 6 3/1/2023 (Wed) PDF Data Extraction rOpenSci. (n.d.). Introduction to tabulizer. rOpenSci. (n.d.). Using the Tesseract OCR engine in R. 6 3/3/2023 (Fri) Lab 5, Quiz 2 Open 7 3/6/2023 (Mon) Bash Irizarry, R. A. (2022). Chapter 39 Organizing with Unix. In Introduction to Data Science. Recap Survey 7 3/8/2023 (Wed) Remote Servers Tanner, A. (2022, September 18). Working with remote computers. Lab 5 7 3/10/2023 (Fri) RECAP/MSA Quiz 2 Due 8 3/13/2023 (Mon) No Class 8 3/15/2023 (Wed) No Class 8 3/17/2023 (Fri) No Class 9 3/20/2023 (Mon) Package Creation [Skim] Wickham, H., \u0026amp; Bryan, J. (2023). 2 The Whole Game. In R Packages (2nd ed.). O'Reilly. Final Project Ideas 9 3/22/2023 (Wed) Project Management Quickstart for GitHub Issues. (n.d.). GitHub Docs. Final Project Interest Form 9 3/24/2023 (Fri) Finals Work Time 1 Final Project Ranking 10 3/27/2023 (Mon) Adv Git Community, T. T. W. (2022a). Git Branches. In The Turing Way: A handbook for reproducible, ethical and collaborative research. Community, T. T. W. (2022b). Merging Branches in Git. In The Turing Way: A handbook for reproducible, ethical and collaborative research. Community, T. T. W. (2022c). Retrieving and Comparing Versions. In The Turing Way: A handbook for reproducible, ethical and collaborative research. 10 3/29/2023 (Wed) Package Documentation Wickham, H., \u0026amp; Bryan, J. (2023). 17 Function documentation. In R Packages (2nd ed.). O'Reilly. Wickham, H., \u0026amp; Bryan, J. (2023). 19 Other markdown files. In R Packages (2nd ed.). O'Reilly. 10 3/31/2023 (Fri) Lab 6 11 4/3/2023 (Mon) Adv Functions 11 4/5/2023 (Wed) S3 Lab 6 11 4/7/2023 (Fri) Lab 7, Quiz 3 Open 12 4/10/2023 (Mon) Testing Wickham, H., \u0026amp; Bryan, J. (2023). 14 Testing basics. In R Packages (2nd ed.). O'Reilly. Wickham, H., \u0026amp; Bryan, J. (2023). 15 Designing your test suite. In R Packages (2nd ed.). O'Reilly. 12 4/12/2023 (Wed) Benchmarking Wickham, H. (2020). 23 Measuring performance. In Advanced R. Chapman \u0026amp; Hall. Lab 7 12 4/14/2023 (Fri) Lab 8 Quiz 3 Due 13 4/17/2023 (Mon) Package Vignettes/Reprex Wickham, H., \u0026amp; Bryan, J. (2023). 18 Vignettes. In R Packages (2nd ed.). O'Reilly. 13 4/19/2023 (Wed) Package Website Wickham, H., \u0026amp; Bryan, J. (2023). 20 Website. In R Packages (2nd ed.). O'Reilly. Lab 8 13 4/21/2023 (Fri) Lab 9 14 4/24/2023 (Mon) Student's Choice 14 4/26/2023 (Wed) Student's Choice Lab 9 14 4/28/2023 (Fri) Finals Work Time 2, Quiz 4 Open Final Standards Draft 15 4/1/2023 (Mon) Finals Work Time 3 15 4/3/2023 (Wed) Finals Showcase Final Presentation 15 4/5/2023 (Fri) No Class Quiz 4 Due "},{"uri":"https://Adv-R-Programming.github.io/Adv-R-Reader/tags/","title":"Tags","tags":[],"description":"","content":""}]